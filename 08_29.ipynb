{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acorn\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# backend = tensorflow = K\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "x = np.random.rand(3, 3)\n",
    "y = np.random.rand(3, 2)\n",
    "# initialize 불필요\n",
    "x_var = K.variable(value=x)\n",
    "y_var = K.variable(value=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = K.dot(x_var, y_var)\n",
    "matrix_product = K.eval(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of word\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                     'Sweden is best',\n",
    "                     'Germany beats both'])\n",
    "count = CountVectorizer(token_pattern=r'[a-zA-Z]+')\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'i', 'is', 'love', 'sweden']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = count.get_feature_names()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beats</th>\n",
       "      <th>best</th>\n",
       "      <th>both</th>\n",
       "      <th>brazil</th>\n",
       "      <th>germany</th>\n",
       "      <th>i</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>sweden</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beats  best  both  brazil  germany  i  is  love  sweden\n",
       "0      0     0     0       2        0  1   0     1       0\n",
       "1      0     1     0       0        0  0   1     0       1\n",
       "2      1     0     1       0        1  0   0     0       0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(bag_of_words.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acorn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 한글인 경우 stopword 단어장을 만들어 사용해야 한다.\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenized_words = ['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'trandit', 'meet']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어근 추출\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'tranditional', 'meeting']\n",
    "porter = PorterStemmer()\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                     'Sweden is best',\n",
    "                     'Germany beats both'])\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beats', 'best', 'both', 'brazil', 'germany', 'is', 'love', 'sweden']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'science',\n",
       " 'of',\n",
       " 'today',\n",
       " 'is',\n",
       " 'the',\n",
       " 'technology',\n",
       " 'of',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'Tomorrow',\n",
       " 'is',\n",
       " 'today',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "string = \"The science of today is the technology of tomorrow. Tomorrow is today.\"\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.', 'Tomorrow is today.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "# 다차원 벡터를 -> matrix -> 저차원 벡터\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(777)\n",
    "top_words = 5000\n",
    "# imdb 영화 평가 데이터 negative/positive :target\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (1,) but got array with shape (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-167873587fb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# fit() return : history = epoch마다의 --- 찾아보도록\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;31m# evaluate() return : cost, accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    948\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    785\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    135\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    138\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (1,) but got array with shape (10,)"
     ]
    }
   ],
   "source": [
    "model.add(Conv1D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "# positive/negative\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "# fit() return : history = epoch마다의 --- 찾아보도록\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# evaluate() return : cost, accuracy\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist를 keras RNN으로\n",
    "# RNN : base RNN\n",
    "# simple RNN : output이 input으로 feedback\n",
    "# LSTM : 알아서 알아봐라\n",
    "# GRU : LSTM 속도 개선\n",
    "import keras\n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import SimpleRNN\n",
    "from keras import initializers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "hidden_units = 100\n",
    "learning_rate = 1e-6\n",
    "clip_norm = 1.0\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], -1, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], -1, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "차원 : (60000, 784, 1)\n"
     ]
    }
   ],
   "source": [
    "print('차원 :', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_units, kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
    "                   recurrent_initializer=initializers.Identity(gain=1.0),\n",
    "                   activation='relu',\n",
    "                   input_shape=x_train.shape[1:]))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "rmsprop = RMSprop(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=rmsprop,\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_13 (SimpleRNN)    (None, 100)               10200     \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                1010      \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 11,210\n",
      "Trainable params: 11,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 180s 3ms/step - loss: 2.1516 - acc: 0.1941 - val_loss: 2.0119 - val_acc: 0.2308\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 176s 3ms/step - loss: 1.9808 - acc: 0.2369 - val_loss: 1.9649 - val_acc: 0.2420\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 176s 3ms/step - loss: 1.9467 - acc: 0.2442 - val_loss: 1.9323 - val_acc: 0.2569\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 178s 3ms/step - loss: 1.9096 - acc: 0.2526 - val_loss: 1.8789 - val_acc: 0.2606\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 180s 3ms/step - loss: 1.8464 - acc: 0.2868 - val_loss: 1.8010 - val_acc: 0.3172\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 177s 3ms/step - loss: 1.7565 - acc: 0.3468 - val_loss: 1.7209 - val_acc: 0.3669\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 176s 3ms/step - loss: 1.6907 - acc: 0.3905 - val_loss: 1.6792 - val_acc: 0.4046\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 176s 3ms/step - loss: 1.6521 - acc: 0.4096 - val_loss: 1.6233 - val_acc: 0.4349\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 177s 3ms/step - loss: 1.6228 - acc: 0.4220 - val_loss: 1.6114 - val_acc: 0.4326\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 177s 3ms/step - loss: 1.5953 - acc: 0.4293 - val_loss: 1.5780 - val_acc: 0.4433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x503ad208>"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         verbose=1,\n",
    "         validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"test score:\", scores[0])\n",
    "print(\"test accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 5, 5)              140       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 1)              6         \n",
      "=================================================================\n",
      "Total params: 146\n",
      "Trainable params: 146\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      " - 1s - loss: 0.1929\n",
      "Epoch 2/500\n",
      " - 0s - loss: 0.1901\n",
      "Epoch 3/500\n",
      " - 0s - loss: 0.1873\n",
      "Epoch 4/500\n",
      " - 0s - loss: 0.1845\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.1817\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.1790\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.1763\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.1736\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.1709\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.1683\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.1657\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.1631\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.1606\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.1581\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.1556\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.1531\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.1507\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.1483\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.1459\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.1436\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.1413\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.1390\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.1367\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.1345\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.1323\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.1301\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.1279\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.1258\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.1237\n",
      "Epoch 30/500\n",
      " - 0s - loss: 0.1216\n",
      "Epoch 31/500\n",
      " - 0s - loss: 0.1196\n",
      "Epoch 32/500\n",
      " - 0s - loss: 0.1175\n",
      "Epoch 33/500\n",
      " - 0s - loss: 0.1155\n",
      "Epoch 34/500\n",
      " - 0s - loss: 0.1136\n",
      "Epoch 35/500\n",
      " - 0s - loss: 0.1116\n",
      "Epoch 36/500\n",
      " - 0s - loss: 0.1097\n",
      "Epoch 37/500\n",
      " - 0s - loss: 0.1078\n",
      "Epoch 38/500\n",
      " - 0s - loss: 0.1059\n",
      "Epoch 39/500\n",
      " - 0s - loss: 0.1041\n",
      "Epoch 40/500\n",
      " - 0s - loss: 0.1022\n",
      "Epoch 41/500\n",
      " - 0s - loss: 0.1004\n",
      "Epoch 42/500\n",
      " - 0s - loss: 0.0986\n",
      "Epoch 43/500\n",
      " - 0s - loss: 0.0969\n",
      "Epoch 44/500\n",
      " - 0s - loss: 0.0952\n",
      "Epoch 45/500\n",
      " - 0s - loss: 0.0934\n",
      "Epoch 46/500\n",
      " - 0s - loss: 0.0917\n",
      "Epoch 47/500\n",
      " - 0s - loss: 0.0901\n",
      "Epoch 48/500\n",
      " - 0s - loss: 0.0884\n",
      "Epoch 49/500\n",
      " - 0s - loss: 0.0868\n",
      "Epoch 50/500\n",
      " - 0s - loss: 0.0852\n",
      "Epoch 51/500\n",
      " - 0s - loss: 0.0836\n",
      "Epoch 52/500\n",
      " - 0s - loss: 0.0821\n",
      "Epoch 53/500\n",
      " - 0s - loss: 0.0805\n",
      "Epoch 54/500\n",
      " - 0s - loss: 0.0790\n",
      "Epoch 55/500\n",
      " - 0s - loss: 0.0775\n",
      "Epoch 56/500\n",
      " - 0s - loss: 0.0760\n",
      "Epoch 57/500\n",
      " - 0s - loss: 0.0746\n",
      "Epoch 58/500\n",
      " - 0s - loss: 0.0731\n",
      "Epoch 59/500\n",
      " - 0s - loss: 0.0717\n",
      "Epoch 60/500\n",
      " - 0s - loss: 0.0703\n",
      "Epoch 61/500\n",
      " - 0s - loss: 0.0689\n",
      "Epoch 62/500\n",
      " - 0s - loss: 0.0676\n",
      "Epoch 63/500\n",
      " - 0s - loss: 0.0662\n",
      "Epoch 64/500\n",
      " - 0s - loss: 0.0649\n",
      "Epoch 65/500\n",
      " - 0s - loss: 0.0636\n",
      "Epoch 66/500\n",
      " - 0s - loss: 0.0623\n",
      "Epoch 67/500\n",
      " - 0s - loss: 0.0611\n",
      "Epoch 68/500\n",
      " - 0s - loss: 0.0598\n",
      "Epoch 69/500\n",
      " - 0s - loss: 0.0586\n",
      "Epoch 70/500\n",
      " - 0s - loss: 0.0574\n",
      "Epoch 71/500\n",
      " - 0s - loss: 0.0562\n",
      "Epoch 72/500\n",
      " - 0s - loss: 0.0550\n",
      "Epoch 73/500\n",
      " - 0s - loss: 0.0539\n",
      "Epoch 74/500\n",
      " - 0s - loss: 0.0527\n",
      "Epoch 75/500\n",
      " - 0s - loss: 0.0516\n",
      "Epoch 76/500\n",
      " - 0s - loss: 0.0505\n",
      "Epoch 77/500\n",
      " - 0s - loss: 0.0494\n",
      "Epoch 78/500\n",
      " - 0s - loss: 0.0483\n",
      "Epoch 79/500\n",
      " - 0s - loss: 0.0473\n",
      "Epoch 80/500\n",
      " - 0s - loss: 0.0462\n",
      "Epoch 81/500\n",
      " - 0s - loss: 0.0452\n",
      "Epoch 82/500\n",
      " - 0s - loss: 0.0442\n",
      "Epoch 83/500\n",
      " - 0s - loss: 0.0432\n",
      "Epoch 84/500\n",
      " - 0s - loss: 0.0422\n",
      "Epoch 85/500\n",
      " - 0s - loss: 0.0413\n",
      "Epoch 86/500\n",
      " - 0s - loss: 0.0403\n",
      "Epoch 87/500\n",
      " - 0s - loss: 0.0394\n",
      "Epoch 88/500\n",
      " - 0s - loss: 0.0385\n",
      "Epoch 89/500\n",
      " - 0s - loss: 0.0376\n",
      "Epoch 90/500\n",
      " - 0s - loss: 0.0367\n",
      "Epoch 91/500\n",
      " - 0s - loss: 0.0359\n",
      "Epoch 92/500\n",
      " - 0s - loss: 0.0350\n",
      "Epoch 93/500\n",
      " - 0s - loss: 0.0342\n",
      "Epoch 94/500\n",
      " - 0s - loss: 0.0333\n",
      "Epoch 95/500\n",
      " - 0s - loss: 0.0325\n",
      "Epoch 96/500\n",
      " - 0s - loss: 0.0317\n",
      "Epoch 97/500\n",
      " - 0s - loss: 0.0310\n",
      "Epoch 98/500\n",
      " - 0s - loss: 0.0302\n",
      "Epoch 99/500\n",
      " - 0s - loss: 0.0294\n",
      "Epoch 100/500\n",
      " - 0s - loss: 0.0287\n",
      "Epoch 101/500\n",
      " - 0s - loss: 0.0280\n",
      "Epoch 102/500\n",
      " - 0s - loss: 0.0273\n",
      "Epoch 103/500\n",
      " - 0s - loss: 0.0266\n",
      "Epoch 104/500\n",
      " - 0s - loss: 0.0259\n",
      "Epoch 105/500\n",
      " - 0s - loss: 0.0252\n",
      "Epoch 106/500\n",
      " - 0s - loss: 0.0246\n",
      "Epoch 107/500\n",
      " - 0s - loss: 0.0239\n",
      "Epoch 108/500\n",
      " - 0s - loss: 0.0233\n",
      "Epoch 109/500\n",
      " - 0s - loss: 0.0227\n",
      "Epoch 110/500\n",
      " - 0s - loss: 0.0221\n",
      "Epoch 111/500\n",
      " - 0s - loss: 0.0215\n",
      "Epoch 112/500\n",
      " - 0s - loss: 0.0209\n",
      "Epoch 113/500\n",
      " - 0s - loss: 0.0203\n",
      "Epoch 114/500\n",
      " - 0s - loss: 0.0198\n",
      "Epoch 115/500\n",
      " - 0s - loss: 0.0193\n",
      "Epoch 116/500\n",
      " - 0s - loss: 0.0187\n",
      "Epoch 117/500\n",
      " - 0s - loss: 0.0182\n",
      "Epoch 118/500\n",
      " - 0s - loss: 0.0177\n",
      "Epoch 119/500\n",
      " - 0s - loss: 0.0172\n",
      "Epoch 120/500\n",
      " - 0s - loss: 0.0168\n",
      "Epoch 121/500\n",
      " - 0s - loss: 0.0163\n",
      "Epoch 122/500\n",
      " - 0s - loss: 0.0159\n",
      "Epoch 123/500\n",
      " - 0s - loss: 0.0155\n",
      "Epoch 124/500\n",
      " - 0s - loss: 0.0150\n",
      "Epoch 125/500\n",
      " - 0s - loss: 0.0146\n",
      "Epoch 126/500\n",
      " - 0s - loss: 0.0143\n",
      "Epoch 127/500\n",
      " - 0s - loss: 0.0139\n",
      "Epoch 128/500\n",
      " - 0s - loss: 0.0135\n",
      "Epoch 129/500\n",
      " - 0s - loss: 0.0132\n",
      "Epoch 130/500\n",
      " - 0s - loss: 0.0128\n",
      "Epoch 131/500\n",
      " - 0s - loss: 0.0125\n",
      "Epoch 132/500\n",
      " - 0s - loss: 0.0122\n",
      "Epoch 133/500\n",
      " - 0s - loss: 0.0119\n",
      "Epoch 134/500\n",
      " - 0s - loss: 0.0116\n",
      "Epoch 135/500\n",
      " - 0s - loss: 0.0113\n",
      "Epoch 136/500\n",
      " - 0s - loss: 0.0111\n",
      "Epoch 137/500\n",
      " - 0s - loss: 0.0108\n",
      "Epoch 138/500\n",
      " - 0s - loss: 0.0106\n",
      "Epoch 139/500\n",
      " - 0s - loss: 0.0104\n",
      "Epoch 140/500\n",
      " - 0s - loss: 0.0101\n",
      "Epoch 141/500\n",
      " - 0s - loss: 0.0099\n",
      "Epoch 142/500\n",
      " - 0s - loss: 0.0097\n",
      "Epoch 143/500\n",
      " - 0s - loss: 0.0096\n",
      "Epoch 144/500\n",
      " - 0s - loss: 0.0094\n",
      "Epoch 145/500\n",
      " - 0s - loss: 0.0092\n",
      "Epoch 146/500\n",
      " - 0s - loss: 0.0091\n",
      "Epoch 147/500\n",
      " - 0s - loss: 0.0089\n",
      "Epoch 148/500\n",
      " - 0s - loss: 0.0088\n",
      "Epoch 149/500\n",
      " - 0s - loss: 0.0086\n",
      "Epoch 150/500\n",
      " - 0s - loss: 0.0085\n",
      "Epoch 151/500\n",
      " - 0s - loss: 0.0084\n",
      "Epoch 152/500\n",
      " - 0s - loss: 0.0083\n",
      "Epoch 153/500\n",
      " - 0s - loss: 0.0082\n",
      "Epoch 154/500\n",
      " - 0s - loss: 0.0081\n",
      "Epoch 155/500\n",
      " - 0s - loss: 0.0080\n",
      "Epoch 156/500\n",
      " - 0s - loss: 0.0079\n",
      "Epoch 157/500\n",
      " - 0s - loss: 0.0078\n",
      "Epoch 158/500\n",
      " - 0s - loss: 0.0078\n",
      "Epoch 159/500\n",
      " - 0s - loss: 0.0077\n",
      "Epoch 160/500\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 161/500\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 162/500\n",
      " - 0s - loss: 0.0075\n",
      "Epoch 163/500\n",
      " - 0s - loss: 0.0074\n",
      "Epoch 164/500\n",
      " - 0s - loss: 0.0074\n",
      "Epoch 165/500\n",
      " - 0s - loss: 0.0073\n",
      "Epoch 166/500\n",
      " - 0s - loss: 0.0073\n",
      "Epoch 167/500\n",
      " - 0s - loss: 0.0072\n",
      "Epoch 168/500\n",
      " - 0s - loss: 0.0072\n",
      "Epoch 169/500\n",
      " - 0s - loss: 0.0072\n",
      "Epoch 170/500\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 171/500\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 172/500\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 173/500\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 174/500\n",
      " - 0s - loss: 0.0070\n",
      "Epoch 175/500\n",
      " - 0s - loss: 0.0069\n",
      "Epoch 176/500\n",
      " - 0s - loss: 0.0069\n",
      "Epoch 177/500\n",
      " - 0s - loss: 0.0068\n",
      "Epoch 178/500\n",
      " - 0s - loss: 0.0068\n",
      "Epoch 179/500\n",
      " - 0s - loss: 0.0068\n",
      "Epoch 180/500\n",
      " - 0s - loss: 0.0067\n",
      "Epoch 181/500\n",
      " - 0s - loss: 0.0067\n",
      "Epoch 182/500\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 183/500\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 184/500\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.0065\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.0065\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.0065\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.0064\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.0064\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.0063\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.0063\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.0063\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.0060\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.0060\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.0059\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.0059\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.0059\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.0054\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.0054\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.0054\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.0053\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.0053\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/500\n",
      " - 0s - loss: 0.0052\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.0052\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.0052\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.0050\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.0050\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.0050\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.0048\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.0048\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.0048\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.0045\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.0045\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.0045\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.0045\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.0037\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 306/500\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.0027\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.0025\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.0023\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.0019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 457/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 458/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.0018\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.0017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f73400>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "length = 5\n",
    "seq = np.array([i/float(length) for i in range(length)])\n",
    "X = seq.reshape(1, length, 1)\n",
    "y = seq.reshape(1, length, 1)\n",
    "\n",
    "n_neurons = length\n",
    "n_batch = 1\n",
    "n_epoch = 500\n",
    "\n",
    "model = Sequential()\n",
    "# Param = 140 = (1 + 5)(state input, input) * 4(Weight) * 5(cell) + 4(bias) * 5(cell)\n",
    "model.add(LSTM(n_neurons, input_shape=(length, 1), return_sequences=True))\n",
    "#model.add(Dense(length))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "print(model.summary())\n",
    "\n",
    "model.fit(X, y, epochs=n_epoch, batch_size=n_batch, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 lines read\n",
      "100 lines read\n",
      "200 lines read\n",
      "300 lines read\n",
      "400 lines read\n",
      "500 lines read\n",
      "600 lines read\n",
      "700 lines read\n",
      "800 lines read\n",
      "900 lines read\n",
      "1000 lines read\n",
      "1100 lines read\n",
      "1200 lines read\n",
      "1300 lines read\n",
      "1400 lines read\n",
      "1500 lines read\n",
      "1600 lines read\n",
      "1700 lines read\n",
      "1800 lines read\n",
      "1900 lines read\n",
      "2000 lines read\n",
      "2100 lines read\n",
      "2200 lines read\n",
      "2300 lines read\n",
      "2400 lines read\n",
      "2500 lines read\n",
      "2600 lines read\n",
      "2700 lines read\n",
      "2800 lines read\n",
      "2900 lines read\n",
      "3000 lines read\n",
      "3100 lines read\n",
      "3200 lines read\n",
      "3300 lines read\n",
      "3400 lines read\n",
      "3500 lines read\n",
      "3600 lines read\n",
      "3700 lines read\n",
      "3800 lines read\n",
      "3900 lines read\n",
      "4000 lines read\n",
      "4100 lines read\n",
      "4200 lines read\n",
      "4300 lines read\n",
      "4400 lines read\n",
      "4500 lines read\n",
      "4600 lines read\n",
      "4700 lines read\n",
      "4800 lines read\n",
      "4900 lines read\n",
      "5000 lines read\n",
      "5100 lines read\n",
      "5200 lines read\n",
      "5300 lines read\n",
      "5400 lines read\n",
      "5500 lines read\n",
      "5600 lines read\n",
      "5700 lines read\n",
      "5800 lines read\n",
      "5900 lines read\n",
      "6000 lines read\n",
      "6100 lines read\n",
      "6200 lines read\n",
      "6300 lines read\n",
      "6400 lines read\n",
      "6500 lines read\n",
      "6600 lines read\n",
      "6700 lines read\n",
      "6800 lines read\n",
      "6900 lines read\n",
      "7000 lines read\n",
      "7100 lines read\n",
      "7200 lines read\n",
      "7300 lines read\n",
      "7400 lines read\n",
      "7500 lines read\n",
      "7600 lines read\n",
      "7700 lines read\n",
      "7800 lines read\n",
      "7900 lines read\n",
      "8000 lines read\n",
      "8100 lines read\n",
      "8200 lines read\n",
      "8300 lines read\n",
      "8400 lines read\n",
      "8500 lines read\n",
      "8600 lines read\n",
      "8700 lines read\n",
      "8800 lines read\n",
      "8900 lines read\n",
      "9000 lines read\n",
      "9100 lines read\n",
      "9200 lines read\n",
      "9300 lines read\n",
      "9400 lines read\n",
      "9500 lines read\n",
      "9600 lines read\n",
      "9700 lines read\n",
      "9800 lines read\n",
      "9900 lines read\n",
      "10000 lines read\n",
      "10100 lines read\n",
      "10200 lines read\n",
      "10300 lines read\n",
      "10400 lines read\n",
      "10500 lines read\n",
      "10600 lines read\n",
      "10700 lines read\n",
      "10800 lines read\n",
      "10900 lines read\n",
      "11000 lines read\n",
      "11100 lines read\n",
      "11200 lines read\n",
      "11300 lines read\n",
      "11400 lines read\n",
      "11500 lines read\n",
      "11600 lines read\n",
      "11700 lines read\n",
      "11800 lines read\n",
      "11900 lines read\n",
      "12000 lines read\n",
      "12100 lines read\n",
      "12200 lines read\n",
      "12300 lines read\n",
      "12400 lines read\n",
      "12500 lines read\n",
      "12600 lines read\n",
      "12700 lines read\n",
      "12800 lines read\n",
      "12900 lines read\n",
      "13000 lines read\n",
      "13100 lines read\n",
      "13200 lines read\n",
      "13300 lines read\n",
      "13400 lines read\n",
      "13500 lines read\n",
      "13600 lines read\n",
      "13700 lines read\n",
      "13800 lines read\n",
      "13900 lines read\n",
      "14000 lines read\n",
      "14100 lines read\n",
      "14200 lines read\n",
      "14300 lines read\n",
      "14400 lines read\n",
      "14500 lines read\n",
      "14600 lines read\n",
      "14700 lines read\n",
      "14800 lines read\n",
      "14900 lines read\n",
      "15000 lines read\n",
      "15100 lines read\n",
      "15200 lines read\n",
      "15300 lines read\n",
      "15400 lines read\n",
      "15500 lines read\n",
      "15600 lines read\n",
      "15700 lines read\n",
      "15800 lines read\n",
      "15900 lines read\n",
      "16000 lines read\n",
      "16100 lines read\n",
      "16200 lines read\n",
      "16300 lines read\n",
      "16400 lines read\n",
      "16500 lines read\n",
      "16600 lines read\n",
      "16700 lines read\n",
      "16800 lines read\n",
      "16900 lines read\n",
      "17000 lines read\n",
      "17100 lines read\n",
      "17200 lines read\n",
      "17300 lines read\n",
      "17400 lines read\n",
      "17500 lines read\n",
      "17600 lines read\n",
      "17700 lines read\n",
      "17800 lines read\n",
      "17900 lines read\n",
      "18000 lines read\n",
      "18100 lines read\n",
      "18200 lines read\n",
      "18300 lines read\n",
      "18400 lines read\n",
      "18500 lines read\n",
      "18600 lines read\n",
      "18700 lines read\n",
      "18800 lines read\n",
      "18900 lines read\n",
      "19000 lines read\n",
      "19100 lines read\n",
      "19200 lines read\n",
      "19300 lines read\n",
      "19400 lines read\n",
      "19500 lines read\n",
      "19600 lines read\n",
      "19700 lines read\n",
      "19800 lines read\n",
      "19900 lines read\n",
      "20000 lines read\n",
      "20100 lines read\n",
      "20200 lines read\n",
      "20300 lines read\n",
      "20400 lines read\n",
      "20500 lines read\n",
      "20600 lines read\n",
      "20700 lines read\n",
      "20800 lines read\n",
      "20900 lines read\n",
      "21000 lines read\n",
      "21100 lines read\n",
      "21200 lines read\n",
      "21300 lines read\n",
      "21400 lines read\n",
      "21500 lines read\n",
      "21600 lines read\n",
      "21700 lines read\n",
      "21800 lines read\n",
      "21900 lines read\n",
      "22000 lines read\n",
      "22100 lines read\n",
      "22200 lines read\n",
      "22300 lines read\n",
      "22400 lines read\n",
      "22500 lines read\n",
      "22600 lines read\n",
      "22700 lines read\n",
      "22800 lines read\n",
      "22900 lines read\n",
      "23000 lines read\n",
      "23100 lines read\n",
      "23200 lines read\n",
      "23300 lines read\n",
      "23400 lines read\n",
      "23500 lines read\n",
      "23600 lines read\n",
      "23700 lines read\n",
      "23800 lines read\n",
      "23900 lines read\n",
      "24000 lines read\n",
      "24100 lines read\n",
      "24200 lines read\n",
      "24300 lines read\n",
      "24400 lines read\n",
      "24500 lines read\n",
      "24600 lines read\n",
      "24700 lines read\n",
      "24800 lines read\n",
      "24900 lines read\n",
      "25000 lines read\n",
      "25100 lines read\n",
      "25200 lines read\n",
      "25300 lines read\n",
      "25400 lines read\n",
      "25500 lines read\n",
      "25600 lines read\n",
      "25700 lines read\n",
      "25800 lines read\n",
      "25900 lines read\n",
      "26000 lines read\n",
      "26100 lines read\n",
      "26200 lines read\n",
      "26300 lines read\n",
      "26400 lines read\n",
      "26500 lines read\n",
      "26600 lines read\n",
      "26700 lines read\n",
      "26800 lines read\n",
      "26900 lines read\n",
      "27000 lines read\n",
      "27100 lines read\n",
      "27200 lines read\n",
      "27300 lines read\n",
      "27400 lines read\n",
      "27500 lines read\n",
      "27600 lines read\n",
      "27700 lines read\n",
      "27800 lines read\n",
      "27900 lines read\n",
      "28000 lines read\n",
      "28100 lines read\n",
      "28200 lines read\n",
      "28300 lines read\n",
      "28400 lines read\n",
      "28500 lines read\n",
      "28600 lines read\n",
      "28700 lines read\n",
      "28800 lines read\n",
      "28900 lines read\n",
      "29000 lines read\n",
      "29100 lines read\n",
      "29200 lines read\n",
      "29300 lines read\n",
      "29400 lines read\n",
      "29500 lines read\n",
      "29600 lines read\n",
      "29700 lines read\n",
      "29800 lines read\n",
      "29900 lines read\n",
      "30000 lines read\n",
      "30100 lines read\n",
      "30200 lines read\n",
      "30300 lines read\n",
      "30400 lines read\n",
      "30500 lines read\n",
      "30600 lines read\n",
      "30700 lines read\n",
      "30800 lines read\n",
      "30900 lines read\n",
      "31000 lines read\n",
      "31100 lines read\n",
      "31200 lines read\n",
      "31300 lines read\n",
      "31400 lines read\n",
      "31500 lines read\n",
      "31600 lines read\n",
      "31700 lines read\n",
      "31800 lines read\n",
      "31900 lines read\n",
      "32000 lines read\n",
      "32100 lines read\n",
      "32200 lines read\n",
      "32300 lines read\n",
      "32400 lines read\n",
      "32500 lines read\n",
      "32600 lines read\n",
      "32700 lines read\n",
      "32800 lines read\n",
      "32900 lines read\n",
      "33000 lines read\n",
      "33100 lines read\n",
      "33200 lines read\n",
      "33300 lines read\n",
      "33400 lines read\n",
      "33500 lines read\n",
      "33600 lines read\n",
      "33700 lines read\n",
      "33800 lines read\n",
      "33900 lines read\n",
      "34000 lines read\n",
      "34100 lines read\n",
      "34200 lines read\n",
      "34300 lines read\n",
      "34400 lines read\n",
      "34500 lines read\n",
      "34600 lines read\n",
      "34700 lines read\n",
      "34800 lines read\n",
      "34900 lines read\n",
      "35000 lines read\n",
      "35100 lines read\n",
      "35200 lines read\n",
      "35300 lines read\n",
      "35400 lines read\n",
      "35500 lines read\n",
      "35600 lines read\n",
      "35700 lines read\n",
      "35800 lines read\n",
      "35900 lines read\n",
      "36000 lines read\n",
      "36100 lines read\n",
      "36200 lines read\n",
      "36300 lines read\n",
      "36400 lines read\n",
      "36500 lines read\n",
      "36600 lines read\n",
      "36700 lines read\n",
      "36800 lines read\n",
      "36900 lines read\n",
      "37000 lines read\n",
      "37100 lines read\n",
      "37200 lines read\n",
      "37300 lines read\n",
      "37400 lines read\n",
      "37500 lines read\n",
      "37600 lines read\n",
      "37700 lines read\n",
      "37800 lines read\n",
      "37900 lines read\n",
      "38000 lines read\n",
      "38100 lines read\n",
      "38200 lines read\n",
      "38300 lines read\n",
      "38400 lines read\n",
      "38500 lines read\n",
      "38600 lines read\n",
      "38700 lines read\n",
      "38800 lines read\n",
      "38900 lines read\n",
      "39000 lines read\n",
      "39100 lines read\n",
      "39200 lines read\n",
      "39300 lines read\n",
      "39400 lines read\n",
      "39500 lines read\n",
      "39600 lines read\n",
      "39700 lines read\n",
      "39800 lines read\n",
      "39900 lines read\n",
      "40000 lines read\n",
      "40100 lines read\n",
      "40200 lines read\n",
      "40300 lines read\n",
      "40400 lines read\n",
      "40500 lines read\n",
      "40600 lines read\n",
      "40700 lines read\n",
      "40800 lines read\n",
      "40900 lines read\n",
      "41000 lines read\n",
      "41100 lines read\n",
      "41200 lines read\n",
      "41300 lines read\n",
      "41400 lines read\n",
      "41500 lines read\n",
      "41600 lines read\n",
      "41700 lines read\n",
      "41800 lines read\n",
      "41900 lines read\n",
      "42000 lines read\n",
      "42100 lines read\n",
      "42200 lines read\n",
      "42300 lines read\n",
      "42400 lines read\n",
      "42500 lines read\n",
      "42600 lines read\n",
      "42700 lines read\n",
      "42800 lines read\n",
      "42900 lines read\n",
      "43000 lines read\n",
      "43100 lines read\n",
      "43200 lines read\n",
      "43300 lines read\n",
      "43400 lines read\n",
      "43500 lines read\n",
      "43600 lines read\n",
      "43700 lines read\n",
      "43800 lines read\n",
      "43900 lines read\n",
      "44000 lines read\n",
      "44100 lines read\n",
      "44200 lines read\n",
      "44300 lines read\n",
      "44400 lines read\n",
      "44500 lines read\n",
      "44600 lines read\n",
      "44700 lines read\n",
      "44800 lines read\n",
      "44900 lines read\n",
      "45000 lines read\n",
      "45100 lines read\n",
      "45200 lines read\n",
      "45300 lines read\n",
      "45400 lines read\n",
      "45500 lines read\n",
      "45600 lines read\n",
      "45700 lines read\n",
      "45800 lines read\n",
      "45900 lines read\n",
      "46000 lines read\n",
      "46100 lines read\n",
      "46200 lines read\n",
      "46300 lines read\n",
      "46400 lines read\n",
      "46500 lines read\n",
      "46600 lines read\n",
      "46700 lines read\n",
      "46800 lines read\n",
      "46900 lines read\n",
      "47000 lines read\n",
      "47100 lines read\n",
      "47200 lines read\n",
      "47300 lines read\n",
      "47400 lines read\n",
      "47500 lines read\n",
      "47600 lines read\n",
      "47700 lines read\n",
      "47800 lines read\n",
      "47900 lines read\n",
      "48000 lines read\n",
      "48100 lines read\n",
      "48200 lines read\n",
      "48300 lines read\n",
      "48400 lines read\n",
      "48500 lines read\n",
      "48600 lines read\n",
      "48700 lines read\n",
      "48800 lines read\n",
      "48900 lines read\n",
      "49000 lines read\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49100 lines read\n",
      "49200 lines read\n",
      "49300 lines read\n",
      "49400 lines read\n",
      "49500 lines read\n",
      "49600 lines read\n",
      "49700 lines read\n",
      "49800 lines read\n",
      "49900 lines read\n",
      "50000 lines read\n",
      "50100 lines read\n",
      "50200 lines read\n",
      "50300 lines read\n",
      "50400 lines read\n",
      "50500 lines read\n",
      "50600 lines read\n",
      "50700 lines read\n",
      "50800 lines read\n",
      "50900 lines read\n",
      "51000 lines read\n",
      "51100 lines read\n",
      "51200 lines read\n",
      "51300 lines read\n",
      "51400 lines read\n",
      "51500 lines read\n",
      "51600 lines read\n",
      "51700 lines read\n",
      "51800 lines read\n",
      "51900 lines read\n",
      "52000 lines read\n",
      "52100 lines read\n",
      "52200 lines read\n",
      "52300 lines read\n",
      "52400 lines read\n",
      "52500 lines read\n",
      "52600 lines read\n",
      "52700 lines read\n",
      "52800 lines read\n",
      "52900 lines read\n",
      "53000 lines read\n",
      "53100 lines read\n",
      "53200 lines read\n",
      "53300 lines read\n",
      "53400 lines read\n",
      "53500 lines read\n",
      "53600 lines read\n",
      "53700 lines read\n",
      "53800 lines read\n",
      "53900 lines read\n",
      "54000 lines read\n",
      "54100 lines read\n",
      "54200 lines read\n",
      "54300 lines read\n",
      "54400 lines read\n",
      "54500 lines read\n",
      "54600 lines read\n",
      "54700 lines read\n",
      "54800 lines read\n",
      "54900 lines read\n",
      "55000 lines read\n",
      "55100 lines read\n",
      "55200 lines read\n",
      "55300 lines read\n",
      "55400 lines read\n",
      "55500 lines read\n",
      "55600 lines read\n",
      "55700 lines read\n",
      "55800 lines read\n",
      "55900 lines read\n",
      "56000 lines read\n",
      "56100 lines read\n",
      "56200 lines read\n",
      "56300 lines read\n",
      "56400 lines read\n",
      "56500 lines read\n",
      "56600 lines read\n",
      "56700 lines read\n",
      "56800 lines read\n",
      "56900 lines read\n",
      "57000 lines read\n",
      "57100 lines read\n",
      "57200 lines read\n",
      "57300 lines read\n",
      "57400 lines read\n",
      "57500 lines read\n",
      "57600 lines read\n",
      "57700 lines read\n",
      "57800 lines read\n",
      "57900 lines read\n",
      "58000 lines read\n",
      "58100 lines read\n",
      "58200 lines read\n",
      "58300 lines read\n",
      "58400 lines read\n",
      "58500 lines read\n",
      "58600 lines read\n",
      "58700 lines read\n",
      "58800 lines read\n",
      "58900 lines read\n",
      "59000 lines read\n",
      "59100 lines read\n",
      "59200 lines read\n",
      "59300 lines read\n",
      "59400 lines read\n",
      "59500 lines read\n",
      "59600 lines read\n",
      "59700 lines read\n",
      "59800 lines read\n",
      "59900 lines read\n",
      "60000 lines read\n",
      "60100 lines read\n",
      "60200 lines read\n",
      "60300 lines read\n",
      "60400 lines read\n",
      "60500 lines read\n",
      "60600 lines read\n",
      "60700 lines read\n",
      "60800 lines read\n",
      "60900 lines read\n",
      "61000 lines read\n",
      "61100 lines read\n",
      "61200 lines read\n",
      "61300 lines read\n",
      "61400 lines read\n",
      "61500 lines read\n",
      "61600 lines read\n",
      "61700 lines read\n",
      "61800 lines read\n",
      "61900 lines read\n",
      "62000 lines read\n",
      "62100 lines read\n",
      "62200 lines read\n",
      "62300 lines read\n",
      "62400 lines read\n",
      "62500 lines read\n",
      "62600 lines read\n",
      "62700 lines read\n",
      "62800 lines read\n",
      "62900 lines read\n",
      "63000 lines read\n",
      "63100 lines read\n",
      "63200 lines read\n",
      "63300 lines read\n",
      "63400 lines read\n",
      "63500 lines read\n",
      "63600 lines read\n",
      "63700 lines read\n",
      "63800 lines read\n",
      "63900 lines read\n",
      "64000 lines read\n",
      "64100 lines read\n",
      "64200 lines read\n",
      "64300 lines read\n",
      "64400 lines read\n",
      "64500 lines read\n",
      "64600 lines read\n",
      "64700 lines read\n",
      "64800 lines read\n",
      "64900 lines read\n",
      "65000 lines read\n",
      "65100 lines read\n",
      "65200 lines read\n",
      "65300 lines read\n",
      "65400 lines read\n",
      "65500 lines read\n",
      "65600 lines read\n",
      "65700 lines read\n",
      "65800 lines read\n",
      "65900 lines read\n",
      "66000 lines read\n",
      "66100 lines read\n",
      "66200 lines read\n",
      "66300 lines read\n",
      "66400 lines read\n",
      "66500 lines read\n",
      "66600 lines read\n",
      "66700 lines read\n",
      "66800 lines read\n",
      "66900 lines read\n",
      "67000 lines read\n",
      "67100 lines read\n",
      "67200 lines read\n",
      "67300 lines read\n",
      "67400 lines read\n",
      "67500 lines read\n",
      "67600 lines read\n",
      "67700 lines read\n",
      "67800 lines read\n",
      "67900 lines read\n",
      "68000 lines read\n",
      "68100 lines read\n",
      "68200 lines read\n",
      "68300 lines read\n",
      "68400 lines read\n",
      "68500 lines read\n",
      "68600 lines read\n",
      "68700 lines read\n",
      "68800 lines read\n",
      "68900 lines read\n",
      "69000 lines read\n",
      "69100 lines read\n",
      "69200 lines read\n",
      "69300 lines read\n",
      "69400 lines read\n",
      "69500 lines read\n",
      "69600 lines read\n",
      "69700 lines read\n",
      "69800 lines read\n",
      "69900 lines read\n",
      "70000 lines read\n",
      "70100 lines read\n",
      "70200 lines read\n",
      "70300 lines read\n",
      "70400 lines read\n",
      "70500 lines read\n",
      "70600 lines read\n",
      "70700 lines read\n",
      "70800 lines read\n",
      "70900 lines read\n",
      "71000 lines read\n",
      "71100 lines read\n",
      "71200 lines read\n",
      "71300 lines read\n",
      "71400 lines read\n",
      "71500 lines read\n",
      "71600 lines read\n",
      "71700 lines read\n",
      "71800 lines read\n",
      "71900 lines read\n",
      "72000 lines read\n",
      "72100 lines read\n",
      "72200 lines read\n",
      "72300 lines read\n",
      "72400 lines read\n",
      "72500 lines read\n",
      "72600 lines read\n",
      "72700 lines read\n",
      "72800 lines read\n",
      "72900 lines read\n",
      "73000 lines read\n",
      "73100 lines read\n",
      "73200 lines read\n",
      "73300 lines read\n",
      "73400 lines read\n",
      "73500 lines read\n",
      "73600 lines read\n",
      "73700 lines read\n",
      "73800 lines read\n",
      "73900 lines read\n",
      "74000 lines read\n",
      "74100 lines read\n",
      "74200 lines read\n",
      "74300 lines read\n",
      "74400 lines read\n",
      "74500 lines read\n",
      "74600 lines read\n",
      "74700 lines read\n",
      "74800 lines read\n",
      "74900 lines read\n",
      "75000 lines read\n",
      "75100 lines read\n",
      "75200 lines read\n",
      "75300 lines read\n",
      "75400 lines read\n",
      "75500 lines read\n",
      "75600 lines read\n",
      "75700 lines read\n",
      "75800 lines read\n",
      "75900 lines read\n",
      "76000 lines read\n",
      "76100 lines read\n",
      "76200 lines read\n",
      "76300 lines read\n",
      "76400 lines read\n",
      "76500 lines read\n",
      "76600 lines read\n",
      "76700 lines read\n",
      "76800 lines read\n",
      "76900 lines read\n",
      "77000 lines read\n",
      "77100 lines read\n",
      "77200 lines read\n",
      "77300 lines read\n",
      "77400 lines read\n",
      "77500 lines read\n",
      "77600 lines read\n",
      "77700 lines read\n",
      "77800 lines read\n",
      "77900 lines read\n",
      "78000 lines read\n",
      "78100 lines read\n",
      "78200 lines read\n",
      "78300 lines read\n",
      "78400 lines read\n",
      "78500 lines read\n",
      "78600 lines read\n",
      "78700 lines read\n",
      "78800 lines read\n",
      "78900 lines read\n",
      "79000 lines read\n",
      "79100 lines read\n",
      "79200 lines read\n",
      "79300 lines read\n",
      "79400 lines read\n",
      "79500 lines read\n",
      "79600 lines read\n",
      "79700 lines read\n",
      "79800 lines read\n",
      "79900 lines read\n",
      "80000 lines read\n",
      "80100 lines read\n",
      "80200 lines read\n",
      "80300 lines read\n",
      "80400 lines read\n",
      "80500 lines read\n",
      "80600 lines read\n",
      "80700 lines read\n",
      "80800 lines read\n",
      "80900 lines read\n",
      "81000 lines read\n",
      "81100 lines read\n",
      "81200 lines read\n",
      "81300 lines read\n",
      "81400 lines read\n",
      "81500 lines read\n",
      "81600 lines read\n",
      "81700 lines read\n",
      "81800 lines read\n",
      "81900 lines read\n",
      "82000 lines read\n",
      "82100 lines read\n",
      "82200 lines read\n",
      "82300 lines read\n",
      "82400 lines read\n",
      "82500 lines read\n",
      "82600 lines read\n",
      "82700 lines read\n",
      "82800 lines read\n",
      "82900 lines read\n",
      "83000 lines read\n",
      "83100 lines read\n",
      "83200 lines read\n",
      "83300 lines read\n",
      "83400 lines read\n",
      "83500 lines read\n",
      "83600 lines read\n",
      "83700 lines read\n",
      "83800 lines read\n",
      "83900 lines read\n",
      "84000 lines read\n",
      "84100 lines read\n",
      "84200 lines read\n",
      "84300 lines read\n",
      "84400 lines read\n",
      "84500 lines read\n",
      "84600 lines read\n",
      "84700 lines read\n",
      "84800 lines read\n",
      "84900 lines read\n",
      "85000 lines read\n",
      "85100 lines read\n",
      "85200 lines read\n",
      "85300 lines read\n",
      "85400 lines read\n",
      "85500 lines read\n",
      "85600 lines read\n",
      "85700 lines read\n",
      "85800 lines read\n",
      "85900 lines read\n",
      "86000 lines read\n",
      "86100 lines read\n",
      "86200 lines read\n",
      "86300 lines read\n",
      "86400 lines read\n",
      "86500 lines read\n",
      "86600 lines read\n",
      "86700 lines read\n",
      "86800 lines read\n",
      "86900 lines read\n",
      "87000 lines read\n",
      "87100 lines read\n",
      "87200 lines read\n",
      "87300 lines read\n",
      "87400 lines read\n",
      "87500 lines read\n",
      "87600 lines read\n",
      "87700 lines read\n",
      "87800 lines read\n",
      "87900 lines read\n",
      "88000 lines read\n",
      "88100 lines read\n",
      "88200 lines read\n",
      "88300 lines read\n",
      "88400 lines read\n",
      "88500 lines read\n",
      "88600 lines read\n",
      "88700 lines read\n",
      "88800 lines read\n",
      "88900 lines read\n",
      "89000 lines read\n",
      "89100 lines read\n",
      "89200 lines read\n",
      "89300 lines read\n",
      "89400 lines read\n",
      "89500 lines read\n",
      "89600 lines read\n",
      "89700 lines read\n",
      "89800 lines read\n",
      "89900 lines read\n",
      "90000 lines read\n",
      "90100 lines read\n",
      "90200 lines read\n",
      "90300 lines read\n",
      "90400 lines read\n",
      "90500 lines read\n",
      "90600 lines read\n",
      "90700 lines read\n",
      "90800 lines read\n",
      "90900 lines read\n",
      "91000 lines read\n",
      "91100 lines read\n",
      "91200 lines read\n",
      "91300 lines read\n",
      "91400 lines read\n",
      "91500 lines read\n",
      "91600 lines read\n",
      "91700 lines read\n",
      "91800 lines read\n",
      "91900 lines read\n",
      "92000 lines read\n",
      "92100 lines read\n",
      "92200 lines read\n",
      "92300 lines read\n",
      "92400 lines read\n",
      "92500 lines read\n",
      "92600 lines read\n",
      "92700 lines read\n",
      "92800 lines read\n",
      "92900 lines read\n",
      "93000 lines read\n",
      "93100 lines read\n",
      "93200 lines read\n",
      "93300 lines read\n",
      "93400 lines read\n",
      "93500 lines read\n",
      "93600 lines read\n",
      "93700 lines read\n",
      "93800 lines read\n",
      "93900 lines read\n",
      "94000 lines read\n",
      "94100 lines read\n",
      "94200 lines read\n",
      "94300 lines read\n",
      "94400 lines read\n",
      "94500 lines read\n",
      "94600 lines read\n",
      "94700 lines read\n",
      "94800 lines read\n",
      "94900 lines read\n",
      "95000 lines read\n",
      "95100 lines read\n",
      "95200 lines read\n",
      "95300 lines read\n",
      "95400 lines read\n",
      "95500 lines read\n",
      "95600 lines read\n",
      "95700 lines read\n",
      "95800 lines read\n",
      "95900 lines read\n",
      "96000 lines read\n",
      "96100 lines read\n",
      "96200 lines read\n",
      "96300 lines read\n",
      "96400 lines read\n",
      "96500 lines read\n",
      "96600 lines read\n",
      "96700 lines read\n",
      "96800 lines read\n",
      "96900 lines read\n",
      "97000 lines read\n",
      "97100 lines read\n",
      "97200 lines read\n",
      "97300 lines read\n",
      "97400 lines read\n",
      "97500 lines read\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97600 lines read\n",
      "97700 lines read\n",
      "97800 lines read\n",
      "97900 lines read\n",
      "98000 lines read\n",
      "98100 lines read\n",
      "98200 lines read\n",
      "98300 lines read\n",
      "98400 lines read\n",
      "98500 lines read\n",
      "98600 lines read\n",
      "98700 lines read\n",
      "98800 lines read\n",
      "98900 lines read\n",
      "99000 lines read\n",
      "99100 lines read\n",
      "99200 lines read\n",
      "99300 lines read\n",
      "99400 lines read\n",
      "99500 lines read\n",
      "99600 lines read\n",
      "99700 lines read\n",
      "99800 lines read\n",
      "99900 lines read\n",
      "100000 lines read\n",
      "100100 lines read\n",
      "100200 lines read\n",
      "100300 lines read\n",
      "100400 lines read\n",
      "100500 lines read\n",
      "100600 lines read\n",
      "100700 lines read\n",
      "100800 lines read\n",
      "100900 lines read\n",
      "101000 lines read\n",
      "101100 lines read\n",
      "101200 lines read\n",
      "101300 lines read\n",
      "101400 lines read\n",
      "101500 lines read\n",
      "101600 lines read\n",
      "101700 lines read\n",
      "101800 lines read\n",
      "101900 lines read\n",
      "102000 lines read\n",
      "102100 lines read\n",
      "102200 lines read\n",
      "102300 lines read\n",
      "102400 lines read\n",
      "102500 lines read\n",
      "102600 lines read\n",
      "102700 lines read\n",
      "102800 lines read\n",
      "102900 lines read\n",
      "103000 lines read\n",
      "103100 lines read\n",
      "103200 lines read\n",
      "103300 lines read\n",
      "103400 lines read\n",
      "103500 lines read\n",
      "103600 lines read\n",
      "103700 lines read\n",
      "103800 lines read\n",
      "103900 lines read\n",
      "104000 lines read\n",
      "104100 lines read\n",
      "104200 lines read\n",
      "104300 lines read\n",
      "104400 lines read\n",
      "104500 lines read\n",
      "104600 lines read\n",
      "104700 lines read\n",
      "104800 lines read\n",
      "104900 lines read\n",
      "105000 lines read\n",
      "105100 lines read\n",
      "105200 lines read\n",
      "105300 lines read\n",
      "105400 lines read\n",
      "105500 lines read\n",
      "105600 lines read\n",
      "105700 lines read\n",
      "105800 lines read\n",
      "105900 lines read\n",
      "106000 lines read\n",
      "106100 lines read\n",
      "106200 lines read\n",
      "106300 lines read\n",
      "106400 lines read\n",
      "106500 lines read\n",
      "106600 lines read\n",
      "106700 lines read\n",
      "106800 lines read\n",
      "106900 lines read\n",
      "107000 lines read\n",
      "107100 lines read\n",
      "107200 lines read\n",
      "107300 lines read\n",
      "107400 lines read\n",
      "107500 lines read\n",
      "107600 lines read\n",
      "107700 lines read\n",
      "107800 lines read\n",
      "107900 lines read\n",
      "108000 lines read\n",
      "108100 lines read\n",
      "108200 lines read\n",
      "108300 lines read\n",
      "108400 lines read\n",
      "108500 lines read\n",
      "108600 lines read\n",
      "108700 lines read\n",
      "108800 lines read\n",
      "108900 lines read\n",
      "109000 lines read\n",
      "109100 lines read\n",
      "109200 lines read\n",
      "109300 lines read\n",
      "109400 lines read\n",
      "109500 lines read\n",
      "109600 lines read\n",
      "109700 lines read\n",
      "109800 lines read\n",
      "109900 lines read\n",
      "110000 lines read\n",
      "110100 lines read\n",
      "110200 lines read\n",
      "110300 lines read\n",
      "110400 lines read\n",
      "110500 lines read\n",
      "110600 lines read\n",
      "110700 lines read\n",
      "110800 lines read\n",
      "110900 lines read\n",
      "111000 lines read\n",
      "111100 lines read\n",
      "111200 lines read\n",
      "111300 lines read\n",
      "111400 lines read\n",
      "111500 lines read\n",
      "111600 lines read\n",
      "111700 lines read\n",
      "111800 lines read\n",
      "111900 lines read\n",
      "112000 lines read\n",
      "112100 lines read\n",
      "112200 lines read\n",
      "112300 lines read\n",
      "112400 lines read\n",
      "112500 lines read\n",
      "112600 lines read\n",
      "112700 lines read\n",
      "112800 lines read\n",
      "112900 lines read\n",
      "113000 lines read\n",
      "113100 lines read\n",
      "113200 lines read\n",
      "113300 lines read\n",
      "113400 lines read\n",
      "113500 lines read\n",
      "113600 lines read\n",
      "113700 lines read\n",
      "113800 lines read\n",
      "113900 lines read\n",
      "114000 lines read\n",
      "114100 lines read\n",
      "114200 lines read\n",
      "114300 lines read\n",
      "114400 lines read\n",
      "114500 lines read\n",
      "114600 lines read\n",
      "114700 lines read\n",
      "114800 lines read\n",
      "114900 lines read\n",
      "115000 lines read\n",
      "115100 lines read\n",
      "115200 lines read\n",
      "115300 lines read\n",
      "115400 lines read\n",
      "115500 lines read\n",
      "115600 lines read\n",
      "115700 lines read\n",
      "115800 lines read\n",
      "115900 lines read\n",
      "116000 lines read\n",
      "116100 lines read\n",
      "116200 lines read\n",
      "116300 lines read\n",
      "116400 lines read\n",
      "116500 lines read\n",
      "116600 lines read\n",
      "116700 lines read\n",
      "116800 lines read\n",
      "116900 lines read\n",
      "117000 lines read\n",
      "117100 lines read\n",
      "117200 lines read\n",
      "117300 lines read\n",
      "117400 lines read\n",
      "117500 lines read\n",
      "117600 lines read\n",
      "117700 lines read\n",
      "117800 lines read\n",
      "117900 lines read\n",
      "118000 lines read\n",
      "118100 lines read\n",
      "118200 lines read\n",
      "118300 lines read\n",
      "118400 lines read\n",
      "118500 lines read\n",
      "118600 lines read\n",
      "118700 lines read\n",
      "118800 lines read\n",
      "118900 lines read\n",
      "119000 lines read\n",
      "119100 lines read\n",
      "119200 lines read\n",
      "119300 lines read\n",
      "119400 lines read\n",
      "119500 lines read\n",
      "119600 lines read\n",
      "119700 lines read\n",
      "119800 lines read\n",
      "119900 lines read\n",
      "120000 lines read\n",
      "120100 lines read\n",
      "120200 lines read\n",
      "120300 lines read\n",
      "120400 lines read\n",
      "120500 lines read\n",
      "120600 lines read\n",
      "120700 lines read\n",
      "120800 lines read\n",
      "120900 lines read\n",
      "121000 lines read\n",
      "121100 lines read\n",
      "121200 lines read\n",
      "121300 lines read\n",
      "121400 lines read\n",
      "121500 lines read\n",
      "121600 lines read\n",
      "121700 lines read\n",
      "121800 lines read\n",
      "121900 lines read\n",
      "122000 lines read\n",
      "122100 lines read\n",
      "122200 lines read\n",
      "122300 lines read\n",
      "122400 lines read\n",
      "122500 lines read\n",
      "122600 lines read\n",
      "122700 lines read\n",
      "122800 lines read\n",
      "122900 lines read\n",
      "123000 lines read\n",
      "123100 lines read\n",
      "123200 lines read\n",
      "123300 lines read\n",
      "123400 lines read\n",
      "123500 lines read\n",
      "123600 lines read\n",
      "123700 lines read\n",
      "123800 lines read\n",
      "123900 lines read\n",
      "124000 lines read\n",
      "124100 lines read\n",
      "124200 lines read\n",
      "124300 lines read\n",
      "124400 lines read\n",
      "124500 lines read\n",
      "124600 lines read\n",
      "124700 lines read\n",
      "124800 lines read\n",
      "124900 lines read\n",
      "125000 lines read\n",
      "125100 lines read\n",
      "125200 lines read\n",
      "125300 lines read\n",
      "125400 lines read\n",
      "125500 lines read\n",
      "125600 lines read\n",
      "125700 lines read\n",
      "125800 lines read\n",
      "125900 lines read\n",
      "126000 lines read\n",
      "126100 lines read\n",
      "126200 lines read\n",
      "126300 lines read\n",
      "126400 lines read\n",
      "126500 lines read\n",
      "126600 lines read\n",
      "126700 lines read\n",
      "126800 lines read\n",
      "126900 lines read\n",
      "127000 lines read\n",
      "127100 lines read\n",
      "127200 lines read\n",
      "127300 lines read\n",
      "127400 lines read\n",
      "127500 lines read\n",
      "127600 lines read\n",
      "127700 lines read\n",
      "127800 lines read\n",
      "127900 lines read\n",
      "128000 lines read\n",
      "128100 lines read\n",
      "128200 lines read\n",
      "128300 lines read\n",
      "128400 lines read\n",
      "128500 lines read\n",
      "128600 lines read\n",
      "128700 lines read\n",
      "128800 lines read\n",
      "128900 lines read\n",
      "129000 lines read\n",
      "129100 lines read\n",
      "129200 lines read\n",
      "129300 lines read\n",
      "129400 lines read\n",
      "129500 lines read\n",
      "129600 lines read\n",
      "129700 lines read\n",
      "129800 lines read\n",
      "129900 lines read\n",
      "130000 lines read\n",
      "130100 lines read\n",
      "130200 lines read\n",
      "130300 lines read\n",
      "130400 lines read\n",
      "130500 lines read\n",
      "130600 lines read\n",
      "130700 lines read\n",
      "130800 lines read\n",
      "130900 lines read\n",
      "131000 lines read\n",
      "131100 lines read\n",
      "131200 lines read\n",
      "131300 lines read\n",
      "131400 lines read\n",
      "131500 lines read\n",
      "131600 lines read\n",
      "131700 lines read\n",
      "131800 lines read\n",
      "131900 lines read\n",
      "132000 lines read\n",
      "132100 lines read\n",
      "132200 lines read\n",
      "132300 lines read\n",
      "132400 lines read\n",
      "132500 lines read\n",
      "132600 lines read\n",
      "132700 lines read\n",
      "132800 lines read\n",
      "132900 lines read\n",
      "133000 lines read\n",
      "133100 lines read\n",
      "133200 lines read\n",
      "133300 lines read\n",
      "133400 lines read\n",
      "133500 lines read\n",
      "133600 lines read\n",
      "133700 lines read\n",
      "133800 lines read\n",
      "133900 lines read\n",
      "134000 lines read\n",
      "134100 lines read\n",
      "134200 lines read\n",
      "134300 lines read\n",
      "134400 lines read\n",
      "134500 lines read\n",
      "134600 lines read\n",
      "134700 lines read\n",
      "134800 lines read\n",
      "134900 lines read\n",
      "135000 lines read\n",
      "135100 lines read\n",
      "135200 lines read\n",
      "135300 lines read\n",
      "135400 lines read\n",
      "135500 lines read\n",
      "135600 lines read\n",
      "135700 lines read\n",
      "135800 lines read\n",
      "135900 lines read\n",
      "136000 lines read\n",
      "136100 lines read\n",
      "136200 lines read\n",
      "136300 lines read\n",
      "136400 lines read\n",
      "136500 lines read\n",
      "136600 lines read\n",
      "136700 lines read\n",
      "136800 lines read\n",
      "136900 lines read\n",
      "137000 lines read\n",
      "137100 lines read\n",
      "137200 lines read\n",
      "137300 lines read\n",
      "137400 lines read\n",
      "137500 lines read\n",
      "137600 lines read\n",
      "137700 lines read\n",
      "137800 lines read\n",
      "137900 lines read\n",
      "138000 lines read\n",
      "138100 lines read\n",
      "138200 lines read\n",
      "138300 lines read\n",
      "138400 lines read\n",
      "138500 lines read\n",
      "138600 lines read\n",
      "138700 lines read\n",
      "138800 lines read\n",
      "138900 lines read\n",
      "139000 lines read\n",
      "139100 lines read\n",
      "139200 lines read\n",
      "139300 lines read\n",
      "139400 lines read\n",
      "139500 lines read\n",
      "139600 lines read\n",
      "139700 lines read\n",
      "139800 lines read\n",
      "139900 lines read\n",
      "140000 lines read\n",
      "140100 lines read\n",
      "140200 lines read\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "DATA_DIR = \"C:\\\\Users\\\\acorn\\\\Documents\\\\acorn\"\n",
    "fld = open(os.path.join(DATA_DIR, \"LD2011_2014.txt\"), \"rb\")\n",
    "data = []\n",
    "line_num = 0\n",
    "cid = 250\n",
    "\n",
    "for line in fld:\n",
    "    if line.startswith(b\"\\\"\\\";\"):\n",
    "        continue\n",
    "    if line_num % 100 == 0:\n",
    "        print(\"{:d} lines read\".format(line_num))\n",
    "    cols = [float(re.sub(b\",\", b\".\", x)) for x in line.strip().split(b\";\")[1:]]\n",
    "    data.append(cols[cid])\n",
    "    line_num += 1\n",
    "fld.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXecG8X5/z+P2jX3XnE3YIoNONj0HooJJUBoCZDwDSmQAilfCCEkBPiSkAQCJHRCDZAfIfRuIBTbgG0M7ti4917OV3wnze+P3ZFWq9ndWWlmpbub9+vll0+rlWZXuzvPPJ0YYzAYDAaDwU2s3AdgMBgMhsrECAiDwWAwCDECwmAwGAxCjIAwGAwGgxAjIAwGg8EgxAgIg8FgMAgxAsJgMBgMQrQJCCJ6iIg2ENEcwXs/JyJGRL3s10REdxDRYiL6nIgO1HVcBoPBYJBDpwbxMICT3BuJaDCAEwCscGw+GcAo+99lAO7WeFwGg8FgkCCh64sZY+8R0VDBW7cB+CWA5x3bTgfwKLPSuqcRUTci6s8YW+s3Rq9evdjQoaIhDAaDweDFjBkzNjHGegftp01AiCCi0wCsZox9RkTOtwYCWOl4vcre5isghg4diunTpys/ToPBYGjPENFymf0iExBEVAvgWgBfFb0t2CYsEkVEl8EyQ2GPPfZQdnwGg8FgyCfKKKYRAIYB+IyIlgEYBGAmEfWDpTEMduw7CMAa0Zcwxu5jjI1njI3v3TtQQzIYDAZDkUQmIBhjsxljfRhjQxljQ2EJhQMZY+sAvADgIjuaaSKA7UH+B4PBYDDoRWeY65MApgLYk4hWEdGlPru/AmAJgMUA7gfwQ13HZTAYDAY5dEYxnR/w/lDH3wzA5bqOxWAwGAzhMZnUBoPBYBBiBITBYDAYhBgBYcCb89Zj/Y6mch+GwWCoMIyA6OAwxvDdR6fjG/dOLfehGAyGCsMIiHbA0k27sKFIDYDZ6YjLNzcoPCKDwdAeiLTUhkEPx/zpXQDAslsmhf6sMF3dYDAYYDQIg8FgMHhgBEQHhzGjQxgMBjFGQHRwjHgwGAxeGB9EG+aMv32Iz1ZtK+k7jAJhMBi8MAKiDTNrZWnCwWAwGPwwJqYODjNGJoPB4IEREB0cY2IyGAxeGAFhMBgMBiFGQBgMBoNBiBEQHRxjYjIYDF4YAdHBKZeT+r9fbMR7X2wsy9gGg0EOE+bawSmXBnHxQx8DKK5+lMFgiAajQRgMBoNBiBEQFcbMFVuxqb45svGMC8JgMHhhBESF8fW/T8Fxf/5vZOOZYn0Gg8ELIyAqkO2NLZGNZcSDwWDwwggIQ4envrkVO5qiE8oGQ1vBRDF1cIyFCTjwhjexO50xEVUGgwujQVQo6UxEM7cRENidzpT7EAyGisQIiAolKj+EqeZqMBi8MAKiQtnWsDuScdqSiWno1S/jj68tKPdhGAwdBiMgKpTWqExMbYy/v/tluQ/BYOgwGAFRoWQiWtobMWQwGLwwAqJCyUTkNzWJcgaDwQsjICqUStcg7n73S1ON1WBo52gTEET0EBFtIKI5jm23EtECIvqciP5DRN0c711DRIuJaCERnajruNoKTvmwZGM97py8qKJW+394bQEusiuyRkHG+GQMhsjRqUE8DOAk17Y3AezLGNsfwBcArgEAIhoD4DwA+9if+TsRxTUeW8Xj1CAufWQ6/vzmF9i4U30Rv1JlzkMfLMW67U1qDsaHqDQqg8GQQ5uAYIy9B2CLa9sbjLFW++U0AIPsv08H8BRjrJkxthTAYgAH6zq2toBzQuSaQ31zq9fuRRM2D4Ixhqc+XpF9fcNL8/C9x6arPqwC0h1IQGzY0YTnPl1d7sMwGMpaauM7AJ62/x4IS2BwVtnbOixOi0pdlXWZdjWn1Q8Uct6ds3oHrn52dt62nU3qBZebDiQfcNFDH2PBup04Zq8+6FqTLPfhGDowZXFSE9G1AFoBPME3CXYTTglEdBkRTSei6Rs3ti8nab6PIfc3FxA6NIiwNLUKhJTo6ilk6peb88797QXr9Q5YZtZsa7T+6EBC0VCZRC4giOhiAKcCuJDlZsRVAAY7dhsEYI3o84yx+xhj4xlj43v37q33YCPGKR+cGkSnrAahw8QUDpEsiJE+CbFySwPOv38afvnM59lt33l4ekUIS11k7wPNgtdgCCJSAUFEJwH4XwCnMcYaHG+9AOA8IqoiomEARgGILkSmQnBO1s6onZqU5a/ftVuDgAgpIUSyQOc81rDb0ljmrtmet313a/stsMf9T5UUtWbomGjzQRDRkwCOBtCLiFYBuB5W1FIVgDfJmmmmMca+zxibS0T/AjAPlunpcsaYBoN7ZeOcEJwaxA67cJ+WKCaXDsEYw5Zdu9GzU5XHJwrFgU4NImZ/dWs6/zgjq3ZbBviptedzNLQNtAkIxtj5gs0P+ux/E4CbdB1PWyDPA2ELi20Nu/H+ok0AgBtfno9RfTvjqNFi09ru1gxSidKUwsemLcdvnp+Lt646EiP7dC54PybSIDTJh5Z0JvvdLa6S3JElEjKG5tYMqpPRRV1zod2RIrcMlYnJpK4gRD6Izbvyq7ouXLfD8/Ojf/1qSWMCwDsLNgAAlm9uEOwNkEAaiLapYNS1r+L7j88EUFi8MKrV9YMfLMVe172GTfXqtTcv+KkZ+WAoN0ZAVBBOcw9fIR/35//m7dOlWm3Yo3sO4pacmEhVgNjfoNMHsXhDPYDymZiem2XlI2QjiyKAa4/GxGQoN0ZAVBD5GgQTThCq4+LdjlDuHE94CQjB5lgEd1GLq3rhEX98B7NWbtM+btw+4ZtfmY/9rn9d+3hA7j4w2eOGcmMERIXCWKHdHYB2W3irPRHHPcxGJNAXRNtUI5orX52zVvu4XJOatmQLdkYUWssFQ1QVfQ0GL4yAqCDcGoSoV7LqFqHuiZdrLXGBBpHOMJx1z5SC7R7KhnbqUvoLAbgFZRRmHz6CcVIbyo0REBVEvg8CaBHE+uteVfIJMBEvnPXrm1rF+QdEWLW1AZc+/ImWZD4valP6I4vcvhgduShuymliak1n8IPHZ2DO6u3BOxvaPUZAVCg3vDRXqEG89Pka/L/pK30/+9AHS/H+IrkyJM45qCWdyQoIYW6Dh6YQI+DW1xdi8oINeGPeOqlxVVBbBg2iQUc9LA/KUeJ82eYGvDpnHX781KeRj22oPIyAqCCck/XKLY1YtL6+YJ/nZq3BLxxlJ0Tc8NI8fOvB8InoHy7ehM9WWSvHMFMTAahOWKv5ppboDOfFaBBPf7ICM5Zvld7fbWqLQoPglMPExE/XWLcMgBEQFYX7mQwyMQzqXlOwLeyq02nWWrk1F8oZpsxDjAjVSetWamqJboVdTPrF//57Ns66u9CPAojP2W1iilaDiGyoLFxzjNK89di05Vi7PbowYoM8RkBUEAUhpwEPaTxGOH3cAHSuzpla3AllwWPm/t7mSMoTfo3HVxPloqui1CBUO4xFP7fbFdMQoQZRDh8EFxBR5WBs2NGE656bg+88rL+niCE8RkBUEO5Hstlnsm1qSYMxy7zT7HAch32w8woEOqOoBN/jFUH1ybKt2WzqKDUI1ZOYaEJ2m5iaIywSWIqA+GDRJmxxZeEHMX3ZFqzaZmXQRyWbuJ9te0O4YzVEgxEQFYT7oWz0mWzrm1vBwEBEeZFFrSXYJZx5FyL7t998fM9/vwQAvL9oY2RCQvUKW3TObmd9lAKiWAHYms7gmw9+hAsf+CjU586+ZyouuN/6TFSVZPkwusq1GErDCIhKwvVM+plrtjW0YOWWxgI7vLskReCQjonAma0smh9kJuSZK7bhdy/ODXUMQcflhWoriGjIzq7SJs2ihkmaKFYA8k8tWr9T+jOrtubX3ooqgIqbRFdva0SrIGrPUF6MgKgg3CacL3we8FPueB9AYRZzaB+E42+nJiKanNbvaPL8Hqeg+kIQfRUWmblRp4mpqSWNOau3Y8PO/HP2M/upYEdTi+N4ivuOYuTK4X94J+91VBFUznvutre+CP35ppa0731ZSWyqb25zja6MgKgg3M/kw1OWee7LHywi4IhRvbLbQ/sgHLs7TUOir5l0xwee39O5KucoV5FZLbN6Vm1i4ue8vbEFN748D6fe+UG21DpHt4nJKYA21xdnl1fxu0RlYnIKiNmrvSsVe3H5EzMx4ebJKg9JG+NvfAtn/u3Dch9GKIyAqCCKeSQJwP0XjccvTtwTQDFRNrlRG3c7BUS4o2lxmLaKbSDU6kjUk5FzOjSInU0tGPu7N/D4tBXCfXSbmJzn9P3HZ+CdhRu0jgeIa35FZWLanc79nsXcNZPt8vRhBdru1kykHfv4fbNoQ+nadZQYAVFBFHPD8hDTAd2qAQDHusqDh8Hp8/Cq8uqF06FerIAYee2r+NaDtpNUQlwqFxAZhh1N/gJWtwbhNu18snRL6O8II9z/8+kqjLq2sI9IVCG2zt+zFD91mHthw84mjP71q3h06vLiBwzJmm1twwzmxgiICqI4DcJ6qkTO6Rtfmhc8puNjzkneHQwlKvvhRSnlv6d8ubnguJycvG+/7N86TExBc5TuCK20u+9FEecY5iOvzBaXRokszNUhIEppXRvG97Zyi5WU959PVxc9Xlic2nlbwgiICqKYh5JPxtsbWwree+CDpcFjOv7OExCugwmzclbRo9pr8j9iVK7dquqgl3SGBa5idWsQ7jDlYuoxhRGcXqcbVR0op4Ao5a6R1SDSGZbNpI+yCjE344mqJFcyRkBUEMWV8rZuuAsm7FHcmJJO6jC2dxUx7V5zXNKR2qxag9idzgQKN91RTO6JrhghGKqOlsfpRmVicmqmxdw2/DOyGoQqjSUsWQHRxvI9jICoJIp4Jvn9VptK4Nzxg0sa3lmqm/sgZq7Yim89+BHqA2zzToIWSXI5DuJ9UoncLavaByFjPtLtpHZPdMVM1CyEUPGaJFWGubamMzjnnimY8mV+RNh/Pl2FK/7prBobfvLknyjmXohyruaCsI3JByMgKoliHknnZOucPOXHzH3eGaPNn7cL7/8I7y/ahOWbG9wfxa1n7y/8zqBVkszc4/W8p+J6BYTXsf35nLEAInBSF2gQRQiIEHdSFBPWxvpmfLJsK37+r8/ytl/5dP7rYo6FCzjZCgJOgRtl9jaP8mtuzeCHT8xoMz4JIyDaOM7noigB4ZhLdjkqld7w0lwwxrJ+CdHq2ukPcBL04AVNXy9+tgYvfLZG+J7zHFWbQZpaMsLvvPXs/XHWQYMwvFdd5AKimHPkXyEz/0XRLpaPESTrijmSbPVZycvy3he5PimR+iAc980rs9fhzfnroxu8BPR3XDFIU8x8lylVg3CM6dQg1u9oxmZHsbddghWPV7RS0IMXNOn96EnvZjWHjeyFSfv3x8ufry0iKdB//+aWtPDYzrFNd6lEDM2ao5iUmJjCfCaCSZLfD0HnUoxPIOeDkJMQP3hiZu6zUZy8jTvXRPd9pAqjQVQQxTipnZ9wml9U4JyARQl4Xg900INeysK/OhnH3y44EKlELLSdPGj3plZvExMAVCXjkWsQYWtrAeGS3LyulFLlLCsgAnYrwcRUjCmulHDssLjDxMOEjZcTIyAqiLJoED5C6bU5uRj5BoEG4eVrCHrw3CvJ1+euC11PJ04U+vdyjjtn9Xa8Mntt3vuT52/A4o3ema5ViVgETmpXmGsR90QYDcLLHFiM5jJr5TbMXlXYy5p/VdBxRRHF5CTKKCZ3L/d/TV+FnU2FoemVhhEQFUQxizZnvHpViSYmN9e/kKvK6r7BAe8HLIzzr6klje89NkOqNPVdFxyQ/TvDGF7+fG2oydA5h5x65wf44RMzscLhfH/ioxX49j8+8fx8VSKG2au2Y+E6+SqpYXFbSooyMYXY18sc2FKE5nLG3z7E1+4qrNfFzyHoXIpxGpeiQZTDSc35bOU2XPufOZGNXyxGQFQQxZTacD4XxWgQsggFhMdwYaKYuGayWKJGTcIxmzW3ZrB6WyPecxXT8x1XMHX+5Glvf4ebqkQcu3anceLt70l/JixuDaKYlbGKRDmVyNbXKuZY+GeKMcWVI1HOybo2UIXWCIgKgj/X3z1imPRnnJNBUoEP4teT9hZuF5lWvLJCwzipwxQXjAsk0uT564UPnwjRvPnpim3S41clc+Nvb2jByi2Fob+loiKKKZSPOoJVtKyJad7aHaEXSSTpABdRjkQ5JwvX7Yy0YGAxGAFRgcRCLG2c91cxTmr3/fm1sQOE+4mcs0U7qR1/f7ay0Gbthej0Hp26HL96drbU50sNi3U2tBl7wxs44o/vlPR9wjEKnNThnZmVpEE0taSxYWczgGDBtWTjLjwzY1Wo7+fPikjTam5NF/TzcJKIUIUQOaW3N7bg8Y/EVYMrBSMgKgj+AIWpg+M0m6hwUnvN7aISE8X6IJwT2OX/nOmzZz5ODeLfPzgk+/fLLmez97jSQ2n5vAxuDUIUHBBEKDnocalUTZ7n3jctW/tIRnD5NckSkfNBFN6fP3lyFg6+ybtXRCIeoQbRKj73BWvD98CIEm0CgogeIqINRDTHsa0HEb1JRIvs/7vb24mI7iCixUT0OREdqOu4Khk+WXOHVq9OqcDPOCeUYkxM7mfWa9IXrYC85pDgUhsyR1aIc9Ia2K02+7fsJFqqOh+FNSBqAeGVCzCgW03ocUV8tjJnwpMRsGFNXvyWEPkgXptrReF5LbhKaN8eGi8zaKWX3tCpQTwM4CTXtqsBTGaMjQIw2X4NACcDGGX/uwzA3RqPq2LhD3YnuzvbCWP6+ext4bz3kwpWRF4CIpwPQs9d7/ze2qp46M+XrgHolxBKBISCUhuqy5gA3hrEz04YnTuekN9JElFMXvkysr4rFUQ5lkq0CQjG2HsA3N1OTgfwiP33IwDOcGx/lFlMA9CNiPrrOrZKhd/GI/rU4anLJuK3p40J/ozj5g/ju3CPmf0Oj68QRTEREd79+dEF2wMTkIrVIBwCsDaZLyB+8lRwNFJb0CDcmtquInoYZ0ttSEy3outNVEzr2tz+Q69+GbNWFjr/3b9fn85VGNitBlccOzK77d73loQaNxvF5CcgPN5r0pzT4sQrMS7KbO5iiNoH0ZcxthYA7P/72NsHAljp2G+Vva0AIrqMiKYT0fSNGzeKdmmz8IeMQJg4vCeqEnFM/tlRvp9x3vvF2I35mL84cU+8ceWRniq+20n9+9P3AQAM7VVXsG8iQEIUV9Y8X2NJuMxpz88S125yUuqiOIp4kzXbGvNW9eFbyIoF4YufrcHIX71SUFNLNEHFiEJnqbvj/N+YW9iIyK1BZBhw5OjeJUVSyeRBeGkuURbMMyam0hD9TMKryhi7jzE2njE2vndvcbG4tgo/YedNM6J3J9/POG/+YmrN80+PGdAFo/t29tzPLSC8CvUB+px/7vMbN7hbqM+XrkGULiIeeH8JLnxgmuf7yzc3YEDXnP2/mNIeornytje/QGuGFYTmim6ZGIVvGOQ2QYq+1/2NGcaEkWlh4IuG3ekMtu7ajfE3vpnn9wC8hUcx5jsAWLZpF8bd8EaoMGeRBt4WiFpArOemI/t/3pF9FQBnM4NBAIKXhO2MYuafHrU5R3Yp3ar4J70mQXdxMT9ZFOQsL3aeLbUb102vzC/p8yrM8je+PB8fLt7s+f7OplZ0rUlmXxeTACZaW3Wqtvxa33t8Bp6dmQslFV1HAoVO0HNPwiI/lHsln86wkhvo8Mi95tYMpi3ZjE31u/H3dxfnj+sxNxfbPvapT1ZiW0OLZ8VhEcVkplcCUQuIFwBcbP99MYDnHdsvsqOZJgLYzk1RHQvbxBTioRncIxfNI5pAg1a97re71abwzYmF3enqXbZwP0d00OEX+6i4NZOwJrVnZ5bWg7hYAbVmWyOu+tcsqTpO6Uwm7zyLcW6K5nYe+LBk4y5c5ejLILrXYjFLg/jzGwsx9UtvYeaH0CTgOq5Mhgn9Zo9MWSY9Dr8Hnp25Co9MXSYcR2Qu61ydKFqD4M+UbDDG0k27PPtfV7iFSWuY65MApgLYk4hWEdGlAG4BcAIRLQJwgv0aAF4BsATAYgD3A/ihruNqC8jeNN8YPwjfPXJ49rVoAgt2NhYKpSuOGVWw1/bGELZwTYsl92rztnPH6RnIg5vO3Leo1q5XPzsbz85cjSkSk21rhuVdx2JKbYjWBHVV4sr+7jvmG+MH4eyDBiHNGO58ezHOv9/bHOY3JhFh3hr/GP80y2kQ//7BodntzhpgQfDf6t2FGzFtiRUT4/7JRM9ATTKODTub8Zvn54QOBMhkBYTc/ne7NBonUdaDKgadUUznM8b6M8aSjLFBjLEHGWObGWPHMcZG2f9vsfdljLHLGWMjGGP7Mcam6zquSibI9HLmATm/fdeaJP549tjsyhAA9h3YteAzQRMMH9N5m4oEzY7G/MqTfhFTQQlRxdry3cc1uEctfnp8oTDTRf+uNbj5zP3ywjJl4FU7O3tM0pxdza14f9Gmgvvg5c/XYspi+ZpT/Pffnc7gHx8uxcdLt+DtBRsCPgVM2q8//nj2WNRVJUILJvfeRMApd7zv+5m0QxgeNKR7UZUAxMmhweVKalNWFNyjU5fjxpfn4f4Q0VPZZ0Zybu/TuVr6uyuNSnFSGyB2Ujv57Wn7ZP8WTeLJeAxdqvMnIdlwReeYou92h+k597jk0KF5NZyC5v+iTUyC6KiaZPh8iFL50XHhhNJOu593teNYRU7gv05eBAAFIaKX/3MmLpCodpv9bscF+N2L8/CNe6d63gfO6/5Hu4VsnCi0k9ot9P3CNxljeGzqMjS3ZvIWGtdLhHW7EZkZC0xMgnNxalRPfrwSN70yH+u2yxXP498ma2JS2d87akxHuQoit5rPv/Em7dcfVYlY3sPstYB334qBGoRgm4yt3flwcMF148uWE1hXSQpR9CxfCZYLxligmYBrEM6JqiWTQVUs/9iLyXkQH1Nxn+PXPREL76QWaRBefLB4E6573jIjOc2GVYnw19Id7gyIneFuulQnC7bJRhrx75c1D/n1fahwC5PRICoJlvUH5G//24UH4i/njssTG543p/0sXHq4VRE2SIMQCSUZ56/fjR2U51B8qY3C27W6DBoEAFx5vGVmkplHuQbhXEnyqJYZy7dmC8qpkqvF/r78mhaVcOka0+8rlmzcldvPsWN1Mvx0JHOoomdAtAj6clNwyXlAbJb1o96+/gcN6S75icrBCAjF1De3Ys5q+QqlToJuPOdk6BUeyFc33DYbVA00m5wXYGJy4ysgAk1Mxc1gIg3i4GE9ivouAHjokvHZv3vUietenXPQIOF2vnCVKUDHo2WcZht+Xc66ewom3VHYZEeErNknzO/rPHyuFbo77RUzpt/q2hle6ryPB3WvFe3uP67gVN2bZE08fs2i8scM56TetTuNvft3weOXTpD7QAVhBIRivvfYdJx65wdFJcYEOb+S8Rhe+tHhAIJNTLy7nKyJKchJHQZdNe5FGsSQnnU4fGQvHLhHuKQ5IN9M9hOBX+Gtq47EreeMFX5WpgaQG+e1cPp0NgrKYZ8mKLv+D8nwzzDWIeeufLIePzRf6E5bIhHqWhDF5L2rM7zUaSEaN7gbLpiwR6AzP29Ywbm6z18kWMcOLgzokCVbyiTAPtS4O42VWxrQks4glYgJNSRTaqODMX3ZVgDFtooUTddivG5OPiy350pPYE4NQsIwKkr84Q5jycja0HgWB4wRislD4t932MieQoEralDk/myYy7ytYXf2b3HiVG7b7eeOwxc3npyn2XywSK60jKyAbmpJY5MtnIDcpP7jY/OFpUy3vwIfhMc9vG57U175ELc5q3NVwrNukQjRc+Y+f5EGsVe/LtJjFHy/wBScybCCvuo/enImjvjjO2hqSSMZI+EzW2qPEt0YAaGJYq67TPgcX/V6zV385pXWIARvy9igqwXhhTOuOx5ViViwDyLw28V4CYh4EaUhrM8RPvvNV/HQJV8R/uh+vhj+VpgIle8/nut90dKayZvIdjW35pt7YoRUIpYXxvzOwo14c976wHFkf4pv/+MTvOH4Pj6BuUNHtzd6O1k57p/Bq4bUxP+bjOccdbPci5FkPBYqOVBmgk1nGJpb0wVa/d79ixMSIg3irncWY8LNk7F4Q312HJ6XsWFnc7a6wKzfnJD3Xc5FQyViBIQmSglt85ueuWDwCrHjNy9/yEWNVJxkV0MhVN3fnDoGPTtVFWyvTSXQuToROEEV+9N4zdfxGBVVnjoWI3StTaIqEReevZ+pjf/+xa4AW9KZvN9pn+tfF07ER47ulff6u49Ox9rtjQHfLndMUz1MR24B4c6BEY+YP+adb3snh210aC3u3zgZjyHD5DVf0W4FRQEzwH6/fQPjb3zTcbzAM98/BMWQK6qZ492FVp7J8X/5L067y/Ip9bT7uazZ1oik/Zt2q03h0+tyQuI5iSKT5cQICIU8Nm15trjafz5djaFXv5z998Gi4EQnJliZuMlqEAFRTLIaBLJjBh5eloHdvZvJEJG0APBqb+qFpwYRo5J7EovOX0pAFBnTuzudQatLeG9rKJyIrzt1DP753QmB+zkpNcy4yiUgNjgmdNW47+P6Zuvc/vHhUqnPi01M+a/TjGF3awY7mvK1Gnd2+eAeck2Ssp0fnaX2HeexYJ3VFY+bB5taMkg67qXuDrNhMcmBUSLlDSKi3wTssoExdo+C42nTPP1Jrr+ssyAaAExesB6Hj+rl/kgeudW8N/w9rwm9MIpJbrYI4yrzcyISJOo/2ed56IieeFGy4NmfzhnrGScfLyJu341I4PoJCP5escO2pllBETlRFdyqRByHjsi/bwKjxEoUEG7T2gqJqqWqCjCu22EJo0emLsP/HDFc8Il8pASE5EUKmwfhfLZE949T0HoVsNydzqAlnZHqBvmdhz/JZsQvufmUosKRwyIbLjARwHnwnkceAdDhBYTzxnTbUWVMODI+CD6ElyO52CgmWe44/wAcMqKn5/sxCQ0ibBw54B9DHisi89dN5+pwOaNZH0SR47YINIgwn/WjVMenW4P9fNU21De35vlD3BQ7onuSa7R9F7I/jWi/1kwG+13/em4fyd9Dtj+zpFauAAAgAElEQVSESIMQ3czOqLukT7/4+qbWPK3CC2e5lNYMQyoCASGr36QZYzsYY9tF/xBNL5WKJ09AuJqUy5hwsjFMPvvmsjjF73e3y39L+yAkzFpXn7xX9u/Txg7w3ZdI/oEMY9byexbisfANboB8TeeUffuHO55sFFNxt/7Z90wtMHnIantBUT4qA2P2GdAFLWmGaQGFBouur+X60YOCMNyI7rWG3WnsdGSli4Q4P94/O8KYm1rkpBI/ZOfiy31/MpZfdDHpcwPvbAqfQV/s4iIssgJCU+Bi+8J5sxZqEME4O8p5jmF/rZcP4t8/OAQ3n7kfqm1zTNCk45W97WSPHvIJTDEiLTeL328Sp+Kc1E5iMcK3Jg7Jvv7WxCHoJXDEZ/fneRAlzMZXPj0r77XfObz848Nx/sFWJdkgU4jKPJQfHD0CALBqq7+ZqXgTU/7rm87cDwCwcksjnp8VXJ5dNK67ZIlIu+STtzMfYnc6I3Uf8Wt/y6sLstFa7uexJc3yTHVuE9IDF43PVjvY4VOKw4uo+kvICogkEXXx+NcVQHkL4lQgLS4JH0aD8JMmuVLD4p2G9KzDBRP2yD4A0sX6fN4LmzgnW801TOSUX+hsPCZnYurXxaqquVe/zvZ3enPxoUN9v4uvfEuRSx8vzW/Z7tcjeZ8BXXHOeCurO0iDkDmmjZKO5/52d7utAY7xYnHfx707V+G4vaxOxD95apboI3mI7gv3sYqE+In79ANQOHHLNBFymsW+//hMvDZnrUBAZFwtcvPfP35M36zAF/XvdsPLsXCCKiSoQtbwOg3AT33ef1XBsbR58jSIAhOTGnshHyJIBec3ZDF5EG7CHHksBnkVIcQX+52HrJM6mSCcecBArN5mhYn6nXuQYOWXM0gwhVnN1weYGnjES6AGIXEB5q/179XA6VmXQpfqRGAuhMougTL2eL9x3cfqvpYXTtgjKxjcxf4aW9KevTM4zkN+74uNeO+LjTjCFYDy8uy1eTkmovtpRO86dKpKSCUi3mFX+uU88MFS/O9Je3nsrY4wMVbk88+AICe1/Of99k0HaBAcngUcWKyPH5+EY/z4vfv6fhdgaQXBGgTfVx6/84hJhrky5n/DOrf7VeAE5PMgwkycfGJ79DsHC9/ngQfBTmrxducktmzzLvFOLrrXptCtNoWtAQldxdbXEnV1c4fZ+o8bjPsaOZ8dt2/gqY9XIAiZa/rLZz7Pe/1vV1QjYC0aiYCHpywLXGh0q8kXmne/+2XwQShAVoOYABPFFIjzRiwwA0jNhtwf4OODkBQQ3P4ZrEFkRYTPPvYeEucQo9IcUhOH90BdKoHJrgY3fgJC1gfBGKSlUlCWrWyYa5iIos27duPgoT1w5OjewveTshqEYMzfnbYPdja14H07H+elz+QK8nWuTqCuKoFdzf6ml2I1iC27CgVPl5rCUtze4wYPvNulzeebfvKF0Z/e+AJXHOvf70M0YnOAg9vLZ8Ad1Es27cLIPp08P9+rk7xWpRITxaQQ54/gjkwIFebqs8+oPp0QjxF+fNxI3+/K+SDCRWaUChFJ2+VFgvCpyw7BWYIKqr4CIkQmtfM6eFUgvf5rYwLNDPzQg8YN66P4eNkWz/d4ZFqwgCjclohT3mToNw4A/HrS3hjdtxNiMUJCQkMLOwH07WIFAIgExDftYAFxt7jw47p7gTu/t3N1AsN61WFgN7kkOUD8+/bq7D+BOxtqOeH+sCCtMIzZTSUmikkhfs+QjJ9XxtzTuTqJL28+Bcfu5W/u4RpEULSD17vDe9UV7CUjQ4gkEuUCBKFIOxrsUwo6bKmNbx9mRY+M7tvZ9/j8iEuGuYY1vYiqynJSkiYm0ZiJGGWdvzL8zxHD8caVRwGwTHjymmg+w3vXCbefZDuJT963X8F7A7vV4KR9+mFoT4noOdewoqq+7tU9d1ADllb2zs+PDlxw+Q4K/8KOQO6ec8NDyEWmNifl6l1topgU4jdZSEUxZSfO0m8GvloMnDg9Juu3f3504XFJHBYheIIVTWA96lLYZ4Bl1nEL0z+etT+61nqbHWTzIKzub8BJ+/bDslsmeYaxykzpsmGuYU0vV/r0u+YmpuYADUKkNCZiMYzq2xnLbpmUjeaSRaYYoue7Hm+MtI9lwnBx0mV1MiaVl+D+elG2vTs6TOTj2H+QfLl40TUNiiryigSsTVmaalD0lGhu0VVW34mJYlJIqZdL1LynWKR9EAF+j8NH9nIUIQ8+MCsPQtLc4/i6mY4CZrK9fp37y1jSQrggpMYEgjN+VT7DOb9H+MnaGWZZFbJzW7HFEAFgl0dV15SgrIiT6mRcKuTUPUmKzFJuDULUl2Hv/l1wxTEj8fd3vYsM5sYs3Oan1fn5D3iJ/CANQjRmOsOE5VlUEqa+gIlWCsDvwZWZ9ILdxfKE9kEIti34/UlIxAivz7XC9aQ0CJKfNL2+L2zeRTwml7DGmP858PdkVmb8EIMma5X1/nnuRVAIvOpjkhEQXl/pNfEF1R6SFhAF31t4geevyw/p9arplUrkKsn63YOiBZCXKXdgtxq8+tMjPL+rJmX9Do1BGoRgzNYMQxFtvENhopgU4vfchQlzVSEh5KOYvN/jLU55lcsDJLq2EYI1iKDpiVeL7dWpCpvqmzG0l9iOzeFRTJYJyf/HU9XBS3Y17/X+wcN6oE/nKrz0+VqMHdwNn0kkS3EzdzGhtc6MeqeT+6R9+uG1uet8vy8eI4keDeJjcmc1c4IEREIyt8X9W4i+99mZ+RnZXmG0ziixmpT3zBtGg+hak0SXam/zaA03MRWhQZRaoFIGWQGRZox5ZtYQkXFSI8CcIKVBcGdw6ZOYbCa1jH9h/0Hd8NZVR2JEb+8wPI5Vi8l/n6BM6tF9O+Otq47CsF51WLqpHiP7iJ3JHO4gzDDLXu45boBoGmWPI1NaJOuDKMI2f883D8TRe/bBy5+vxUufr8UBkgIiLjum4EZ01u5x+jC6+fh2ODGJMGKve9/rY0ECIh6XExDucWWqonpFR2WjxNIZ1Pi4VUVH5VXSJkgbzpmY/JMki/F7qEBWQJgoJgl8ndQSn+c3mUhNDguvJBlci8kiSCgFTdKcMP0g/IQSjwmXGZfPCYGmgQAT0/kHD8ae/TrhoCE9vHeyiUnmQTDBM7xnvy6oTsbx9QMHYmivWhARHp6yLHBMaaEv2LbbcR+0hOyXnpAxMYX6RiCV8L/fkrGY1ARYjIDwNDHZz10xYcTOvKfBPWqwcouVrR9kLZWteCDSGqPQIEwUk0JKVCCyqzx38k4xxLM3XrR5EPFYcU7UUoiFcN76+yBISjgA8j4IkdZSa5sv+HgyPcD5/jLVcoWTiWMCc95fsiG9xWoQXgRqEDErnyZsGXen4KnzMBMFaRCBYcSCk3V+ZrzjHgrq2ZAoQejLVv8tBRPFpBC/B1fGbMQdXX69kGUJn0mtBpmsZtXRefKmF0BVrEW2WF8RiXJu+3YYp3yxv69zAutRl5LqM82JkUyinPj9Qd1r0KMuhcUb6vMc1kECIulYWfv1PXDfv87vvfWcsZj65WY8Nm153j5ev7d0prpgm3OydvYWCRL+ccnnVDRoFCW/TS0mD9Zub8Srs61yBDNXbMWnK7b67t/Uksb6Hd4VMqU0iKyJSYEGwVcmAauMXAP2kocEIF8XyRpTrcNYJpJJZca4zJii36I2mS8gwoT1yqzmRWNO2j/X3vVmu6Q2IJfIl5DwB4h+hgFdq3Hr2WPxwhWH487zD8CQnrXZjOUgM2rYWmIc57MzcXjPwKq8TlRpEM4FQHBJHNnzFGmFlaNBdLgopm/cOxUrtzTiqcsm4rz7pgEA3vn50RjmEVFz25tf+H6fzJyZMzFFp0FAoWMckK2LxMdUg6wwVGncymVSB4woeN9tQgxzvYsx91xxzMi8UhKHjOiJW76+H65+drbUfSnTsU/0PVOuOS7793F798Vxe/fFBfdPw+ptjYFNcrgAackEOIxd4/LfckjPWvSoS2X7QsvAJ+vAhkyCbc7ih2EEPldmihHAleSD6HC1mLiTiQsHADjmT+8W7LeruRWfLNuC9TuaCt5zIrO65SampGw7LR+IKNRkolKDiNzEJKlB8GquKpD2QbjeF5XSCKVBUHDWuHO1eevZ++PnJ+7pOabMpZDJVOdj3vPNg3z3u+xIq8+0X2E6PiYQLPTdK2v+7BRzj2VDl4tIfnTe8inHAiDodyMiOwggfIXeSjIxmSgmD37y1Cycc89UbKz3b8AiY+vnjkRV2ZEyfRKyUUyqnNQS9mqOMqEk6Q8IclKHgST9HvxtrtGJhEEYH0RMojkSnzf++4ujcc74weKdskmBwWOqXGgcvWcfLLtlEgb51NYCclqWu/GWaNz+XXOlQ7h5Jzhvo5BsNFyJQRatmUy2OJ/MsyBV66pMJqayRDER0ZVENJeI5hDRk0RUTUTDiOgjIlpERE8TUXnKF4Zk7prtAIBF6/2bfsiUKmix91ElIGRWJirrPwGS9mqoHVNeg2DKx5StOxXPCgjvfft09m5x6hxXdgKT1Uw6VyV8hVSYlq78W47ZU1yyXBbep0HGB3H0nr0xfkh3ALmyFtscXeV4xVQA2NOjQCMQIreFMd/yGbua01l/hswknohRsKYkeLvUNrsyqIhiIoSIYiKigQB+DGAMY6yRiP4Fy79xCoDbGGNPEdE9AC4FcLfs90bNpvpm9OpUlX0gNgS0cJQxMXENQoWJCZDVIGx/gMLVfDH26lII015V3XlCakz+dlZACCZinix17lc8Vvt541LJpTbcTL/ueN/3w9ZimnfDiXlmlmLIRvdITZyUva59OlvahLN0xQtXHI50xirU6Cc0w9S6EuVS1CTjaGxJo3F3Gl3tnhb1Hpnk7nFlF1VvXXUUjv/LfwEEF21UQbmc1AkANUTUAqAWwFoAxwK4wPF9v0UFCohMhmHumh342l0f4E/njJWOxJF5ZrlanAzRUcsPqQQn+20FkbUA5CurAtZk3btzlXR/ZM8xS8hqLhb5jnL5GoTodunXtRpTrj42z1TiRTwmkRvAr6nPRaXsrswzcYwjE5mWMzFRtkJpKfBoJJkgC+dp8j4eXKMA5PpKAPL3EZi44N+QnrVYsG4n9uzXOSsgdkiEE0v127Df7+IIoZX57lKJvNQGY2w1Ef0JwAoAjQDeADADwDbGGBe3qwAM9BjrMgCXAcAee+whO6wyLv/nTBxr19Wf8uUm6c9JmZgU5kEAVqigfIammjFlVri5kiLWisirXo8sYcqKqHNSywoI638/HwQADJBsWJMIcU39zjW7sJF4cmXqIjmvqQpyGkSw85YIGNO/Cz5ZthXda5N4/5fHFNVgJ5twKXGuooXhPgO64o7zD8DI3p3w+WrL9LxTSoMIvqb8PnKaArdVkIBQ5qQmou4ATgcwDMA2AP8PwMmy38kYuw/AfQAwfvz4yJ3jr85Zh1fnWMXN3EXA/JARECrzIAA52yZHaSZ1CBNT15pkdrVVLNKZ1BLF/GQJ23I0q0GUOG4sRKa6nzklp0FIjBkiOU/VfZSULEHB/UrXThqD08YNwCgfH0MQpUbDfWVo92wTqm72PS3TpEnOB8FNwQ4BEdAnXAXlcFIfD2ApY2wjY6wFwLMADgXQjYi4wBoEYE2I76x4ZGrttGYyIApf7toLKdtm1kmthkQsFsrEpALZMthqTUywxwy38iv1GGQcxlkNQqHZULZhkLox5WuJEVlmJNkyKV7IO6mtMZ1tRC87cnieD2lorzrcf9F43HbuuMBxQ0UbOraFyYYvlrBOaq/L/1qIMVcAmEhEtbBMTMcBmA7gHQBnA3gKwMUAng/xne2CljRT5qAGrIiiwCimgIZBYQmXB6F2NS8TF64y3wOQr4sUlzRJyYwr28VOVe8LKRNTQIXesCQka4mpNBvKO6ktrcWprZz7lcEFz9AJY/zbAnOkntOsrzA3xoRh4m58KpESEIyx36kakDH2ERE9A2AmgFYAn8IyGb0M4CkiutHe9qCqMaMkHiPc+82D8Oa89Xh6+koAQL8u1VgXkEgHWPZWlR2iyqFBxEm+bIDKFS4QnOCkUoWQ9UFko5jicmGxQcSlosSCJ+sQLgg5J3X2iyW+UIIwtcSUmQ1lNVFbg3CO2q0EM6nMc5oRPKiHj+pV9JiylB5uUASMsesBXO/avATAwWU4HKVUJWI4fkxfHDike1ZAnD5ugJSJKc2YdGVPGRIxkgwTbOuZ1Nb/MvkBKkuKABJCCTzwQLJHeNC4IUpvK4tMC+ODUDNk9vcKrIukaDwg15BJ9vd1BpOU4keTiTbkKJwepCiLgGjP/PsHhwLIfzhTiRh2pzOBqx1mzWDKkIqOsP8P2wfac8wwmdRKRgyX4KQuk9r6P7hYn/U/72IWVCk0CJnKqlzD8LvXwghKXnrb//5Va6rkmrRMyKlyTVTGSU2ECcN74tARPTG6b+eSSvSHiWIiADeesa9U8y4VGAGhkC7VCezdvwuA/Bj0VDwGxix1OaiKpcoFglyNF7XL+TBJVeojimQ0CDXkMqnlHmxeAlomccqPMJnqMhqEbKkNwJqsvUyg6jUIHuYavVYok3tBsH6Xf353YsnjyjVkygngb04cUvKYsqjziBry1F3nijwZooywqkkTkLNt8oNWuQorV7E+mbIGqus/yWY1d7ITuErN+ZAJOc2VcFfjgyhHOfVsR8QItcKcL0suikkVYXyFqsyGshgNQiWOa1yTjGP8kO44eFiPbNmB3a0Z1Prk7zCovQGS8XArExWEyqRWMqJ8hJBKwSRbzTUrIOzG9Q0BzemDiIfIuvW7pEeO6o0BXavxg6NGBI4Zk/C3KJb5uSgmCR+Eaq1QxgehyiTLx5Wt5qpKW5LFCAiFOG+reIzwjO2PeNzuaBVkf86UQYNQbRqQSqrSFMUkM67KcF5A3sTENYhSTUzxEIEHfpNY97pUXr8GP3IRRRl4pTwpL/ooHcUU3NZTlqxWKCH0lWsQkmXNo3ZSGxOTQrwmi6wGEWhiUu2DiElHZESaVKV4uRmTNIHoKLUha04bN7grAOD8g0srD1NlBzz4IVNqIwy5EhTe+8hoLWFIZGsxSSyq1AwZzsSkaExAthaTwgFDYDQID7rVJvNKBsvgdQ1TkhEsDOWzbaosgx11JnVC9sEGlD3ZObOW/3585derUxWW3TKp5HGrEjFsrpcL/VQXmWb973ddswsNJSOGc1Iru6ayCw1A6YMajxEaW+SeGZWmLanxIh2tDdGvS3BlTTde91VWQEhoECrXJlL9IOxHW2U118COXPb/qoQSf2hkWjCqGpPsJ0c2UU7Vg50Ko0GouqYSq3nFNR8dmdQSYa6K7yOZRET10YayocsKB5bACAgPilHpvBrA8+J7La0S4XMR2zYzih/sOJFEeQS1jg9Z04BKT6qsY1yUAVsKqXgsWBNVHZkm5aTmZi21ocsyVWSj9mUBqjX9mJymBLWmLRmMgPCgGEeU11yRCGEjVxnFlJCIYgJT+2DHHElVUSFvGlA3mciGuco4jMNQlYijudU/Ekp1XSSpTHXFQimZLdYXYS2mrJPafz8dPgj5arnGxFQRMITvy+D1/GSdqFION5W2zeid1EmJlZ/q1VCoKpyqxpQ0MfHJWpXgTyXkNQh1ZkPrZP00NOU+CMlMapV+u+w1jTAaDrDqdAVq3Yr7bchiBIQHjLFsso4sXhc5bBEwVchV4bT+V/dgB5dpVr0aSkiaBpROJpL2atXx6zICQiZRLgxcg/AV+sqvKU8ulfEHqDVrRRkNB4TVIBQOLIEREB4wBlQJ2gr64XWNwyTgqLz+clnN3PmlZmReSqTFd0Wk1vwUynasuCxD8GSiVoOoSsQCexGrDjyQ0dBUx+nnNIjoIv/C9oNQhVy0odrnVBYjIDxgAA4eWloDEk6olpiKq7nKVsNUNWrOIS/Rm0HRmGE6yqkiFiMQBV9T1UEAqYRV2M1Pc1GvQcj9voD6MNdgDUJDPwgZE5PyaMPotN8wGAHhAWMMNak4Ju3Xv+Tvki9HrT6KSVZ1VeVElQlP1BVlI9NRTuXvm4zFgiew7GpenYkJgL8WoTjTVyYnQXVMAhHJF35UbKpskXlmVEcxRZycJ4sREB5k7AuiotqpjJMPKI8PQnUYZlKijr/q+KaYrABWbTuOk1SUDaBOQNQkrVIXTS3ekUwZxecp0xxJdbADYC1w/EyVuWgtNRARUvGYlNYduQ9CsWNcFiMgPOAXRImAkEzkUulwA+SimDiqbcdSlVWV1e2xm/EEqRCA8gxYWQGsatjalCUgdu32rumUYUx5MTkgwAehOlMOQF0q7lv9VrXwBSwfWqB5VIcPQmKhEXUlV8AICE/4al70TPzf1/cL9V1hOlUpNYHIhM8pjmLK+iD8NAhdJiYZa4SaIQFY5yr7+6p6uGtTVnWcRp+qsAzqq40CkqHLCn/g7rUp33I3qoUvYJXmD9YgVC/k5Eq4R13JFTACwhNmXxCRY/P8g/fAj44dKf1doTpVhTvMwHGjLvedjWLytVerNQ3IxK/rSNyTaemaUbyy5hqEX9nwjOIbSer+VbzQAICutUlsb/QWEOp1Fkvo75ZxjCs2Bcv4J8vhhDACwgPeiMRrzvnZV/fEx7/KL5f8h7PEmoV06WKoj2KKPA8i2+iltNaaYZCJX9cRR56MSzipFWsQNbaA+Jfd79zN8s278OKsNUrNEbkgAJkwV3UDd6tJYmvDbu8xNVxTaR9ExMEkZZIPRkB4wR1RfNV09J69AQAHDeme3cdZh/5/Dh+Gc78iLuUcqgiY4ugIxiQzYBX7IHw1CP6HKhOTjI08O6RCARwiA1aVyYebmJ74aIXw/VP++j7WbG9SbgIBgnwQ1v8qJ7HO1UnUN/n4IDQIpWScJDLV1Ye5yiweo67kCphy355YTiHKahAXHzoUD3/74Lx9+AXrUp3Ar08d4/ldofIgSjhmN86Q05THklK1oy8VxgehOmlNwsSkPEosyMRk/wyqxq1LiRv2cHbZpifVK1xAUkCo/H3jFGCqVDcWJ1kWDSK3kPNqfpTJqF08ymI0CA94sb5cJmzh1eEPDHfMelGOjmey46qI0nIiU2pDNeEyqdUhO5kACsNcAwQER6b0uSwyDZl0aGjJWHAQAKDDbBhcykSpKdheyDX4hC6rDq2VxQgID/gqgSfKjehdV7BPXZX1sF44wb9LWLlqvFTbSVV+IZEc5SYmv/h1xWUZiMj2F0mYtRSSiBPWbm/Cll3ednL1Ya45pd/P8S4b3ixDdcK6z30jp3RoaHF/DU21JgpYUUxBTmpo6AcBAP9duNFvSJMHUUnwC3LuVwZj4Y0nYVD32oJ9alMJfHHjybjyhNG+3yVjAsmNWfwxuxncwzrmFVsafMbkEUVqTUy+dlwN9uo4+Tv6dJhA4rEYZq/ejgN//6bPuGp/31qHBuF3vioFxKAeNQD87yMdWBFF0S00AKAqHgvMg1BtYjpp334A/JMfrdDa6DECwgPeNYqIUJXwVutTiVigZA9jYlLpiNrDFhArfQWE9b+qYbkJxO9m10FQq1MtDk2JUKGsj0fRk1aVyH2RbGvXUulSnUT32iTmr93huY+OI0lKaxAKx0xI1C9TrOnL5g4ZH0QFoXKVICsgFC76AOQm6+aW4LIXqu49mTj9XOSU4kiQiB2a3Jzmh+py387fLMJIYmxtaMHzs9Zg1sptwvf1OKktf4CXKU11BB5gTdYy1XLVRk7ZAiIgyMKYmCoIprBcgXy1UbWTZrawm1RWsyITSNKykfsKCA2TSU0qjkYJrUX1ZBIEv+Y6yiREpUE4mbtmu8c7ak1pgLWgas0w3Pn2YvGIPgEkxdKlOokdTd7Jeda4qjUI22/nI5hUm7VkMQLCA5UFz2QT5aDYzlgVt1bzfv4A1b0DuNbS4FdDR4NBojaV8B2zXOjQljgijVT3JJL0sJXpEPo77Yn60anLhe+r1rgBoHtt0re8B6De3MMXGje8NM9/THVDSmMEhAcqVbpyNSJJJqwv+73Pjae6d0AqEUMyTr4hexyVN3xdVQL1zRJai8JRx/TvEriPjuieMw8YCEB8P+lOpuL3lBsd5p7ALHTF9y4AdK1NYUdTi0TSpXoTkx8dqporEXUjomeIaAERzSeiQ4ioBxG9SUSL7P+7B3+TPnT4IIIbkagvGxCIBjNFTTIeEBJp/a/yXOtScTT4hPPqiHgZN7hb4D46Ko4euIc1rmgSi2ueRLza8OoQwEEhwjr6NHerSYIx+NeAUlzxICnhy+po1Vz/CuA1xtheAMYCmA/gagCTGWOjAEy2X5cNXqxPBWHKfaucSBIOASFy9L0+dx3ueHuxcrNEbSoRMFlz1A1cW5XIZhELx9QQ8VItkbSmwwfh59PSbmLymMx0CGD+vHg9E1oWGnZuU5A/S+XPLKMZWD9FB9AgiKgLgCMBPAgAjLHdjLFtAE4H8Ii92yMAzoj62JyoXCXEYlYjkqDoCNWNXpyIhNP3HpsBQP2Ytam4r5NaB3WpOOoDnIuAYsd4UkZA2OOqrI3kY7LUbWLyWuPoEMA8Ks1TQED9mNVJHvnnv9jQ9TN7Wxk6TqmN4QA2AvgHEX1KRA8QUR2AvoyxtQBg/9+nDMeWRXVqe12VvwmEj6nrLvALAVVt22zJZPDS52s9zUw67PLDetXhy427vMMw1Q2VRc40oP5cYz5h03wFrAuvWH0dPojgXhvqc1t4nskLn63xHldxPwgnXpFpHclJnQBwIIC7GWMHANiFEOYkIrqMiKYT0fSNG71T00uFMXgWziqG2lQCu3ycqNaY+rIl/R421WOu3NIIAHhu1mrh+zpWfqeNGwAA+HDxJvGYijOagVwrWT+05Af4CIgedSl1Azn4jV2M0muhoaOjHC/U5/Uz6xBKPCn29rcWee6jU4Pwco53pES5VQBWMcY+sl8/A0tgrCei/gBg/79B9Ci6PrIAABw/SURBVGHG2H2MsfGMsfG9e/fWdpAZxZO1jAYB6LsJ/DUIPWNWJ6O7vUb36Qwg2hWujENYdblvwL+2V5fqpLJxnJxol4MIWtWr/H1/ceKeAIAJw3oK39dh1nJmqnuhMyfBy0+puo2sLJELCMbYOgAriWhPe9NxAOYBeAHAxfa2iwE8H/WxOVHtE6pN+TtRAb1qpH/zdz1jdqrymKw0hCfGYoQY+a1wlQ2VZWSfToH78Odd5cPt11+EO65vOH0fZeMBubIiQQ2SVDK6b2f0qEt5+noY1KtnVRKLGtX9IJx4ahAoj4mpXP0gfgTgCSJKAVgC4NuwhNW/iOhSACsAnKPzAN77YiP2G9gV3b1Ucqb2oa6riuP9Rf4mMZ2rBN8yFFpGlIh4UT5ecKlmlUKpJhXHWQcOwrQlmz330dE32U+DSGcYjhrdGxcdMlTdgHCWcffQ0DSs5gHrGfSM/tOiQQT7cHQGFPmbmDqABgEAjLFZtplof8bYGYyxrYyxzYyx4xhjo+z/t+gaf3tDCy566GNc8eRMz31Um5gYs/6t3tbou4/qe+DWs/cHINdMRxVj7fyAKHtCAAEtQDUdSjwWfSMdv8TLNGNZH4VKnM2nROgohghY/havyB4dZkOZ+lo62n/+/ox9AXib8HRUH5ChQ2ZSb6xvBgCs2dbkuY9qO+Mlhw4FAGyp9+mxqyE6QqpSpNIRgf870+rN7Xmza5g0Abv6Z8ADpvrBDqwiq6FeUCKbeFn4XmuaZTUMlfASG14CWJcGEfdpx6kjCVGmTDpTPCbgf02t7XquaxAdUkBstgVE91pvh57qpDVuyvLvzQDlT1jQyk8HyaDVpobcACBX/dN3TMW/b4y8V7iAMw9CHfFsba/Cc01nmNwqOCTZ+yji3zceI6Q9Zs2s+U7heL07VwXuw7tNqiSXTOt1rjACIiq4BuEXEqg6aa1bjSWMLv+nt1lLhyMqWygwwlLYcYkxAfWTScrHxKQjtBYojwbhl0mdzujxY/H7yKskde73Vb+yDrIaqjzdPp2rccSoXhjWq7CDZHZcDSamoJYAaQ1CSYYOKSAOHdELADCgW43/jgqvSM9OwSsT1Y5xIFc7R6a3ryqCzFq6dBmryX10TmqAr3C9z4hrUTrCa0WnqssHQUR2zw2PFS4vi6F4RvHTIHTktgBAr05Vvs+LjsJ5XEPzdlIz7XW2RJQriqms9KhLoXNVwqfwGF/1qR2zV6cqDOhW7blPhjEkNN14UYYnytzsWsb1aRqka8ygVqc3vjwfgOqQXut/0biWD0LPus9PAHMtSrUZJO57Te0/VJtlJZpP6YjWAryfmUxGfxkVER1SgwCAeNzPtmn9r3plMm5w18Aywuodt/7hiTqIy5ojNJyrVw9jXWMGaRAclfMmX9gIo5gyejQIwHJUey00+LGoXuXGY+TZaItvVz1mwi8aDpp8hQE9Y4yJKWISMfKewDTErgNWrwS/gn06EnBk252qhEe8RCmUAEtARB6n7zOBOdFRL6i5tTDxsjXDlJaIcZLwiRLLVq1VPHbCJ4opK5QUj+kXDQeoD2ABgp9TxkwUU6QkYjGkI3ZoViXiAd3d9IR+Av79blUTbGKy/lcf0usTEqkh6xYINjFxVD7bvGvfpY9Mx4Yd+aHaGU0+CMBaWXuZXnRpEDEfDU2fUPI+T8BazevQlAAfJ7Wm4IMgOqyAkIqvVnzjpeIxfwGhYQ5PlGE1n3NSe52QHg0tEfD7Aro0iGAfh0ph6Cw9cfDNkzFj+dbs69Z0RttKMxmjQBOTDg3Ce9K0/lc9WScDgh3SGfXnGQ8wMWWY+jFl6LACQkZdVo1lYvKrM69PdY3SSZ0Lc43WxJSKxwLLMqjGr7KqE6UVR131gn77wtzs31p9EIlY4DMTpZNal4kp4aOJAta5yjRrDDWmvZDz87d0pI5yZcfPtslRvcKtSgRoEBrG5Kv5KH0QgTHzmpKq/KJsdDmpnYmIrekMrntujrCcikrB7y5e5yzD0Kox49YZ3fPuwg146IOl2fd0reZTibhn4IEuARGPxZDOME+tMKPBxMQDz7zuXx1jytCBBYSPD0JDCj8g46TWkCiXncCiW83zmHnP+HW+n/Kkqlz0yYYdTfjT6wuz8fm6/B687/dDHy7FJ8u24rFpy/Hzf31WsJ/KW6naLSAct3FGozPTWQzxkn98ghtempc3LqA+D6I6EUOTR3e3XGit2jGTAeaedEZ9HgQPPOD371Mfr8D0ZblydMYHETGWDyK6FH7AEhCtGeZT0lf9jRdUQ0cXibh/LDmgI0osp0H8/JnPcdc7izFjhWWf19EzGchpaH98bWF2YhateFU+3EnXjDh3zQ7MXrUdgDWp6XNSE96Ytx7rtucc463pDP72zmLssNu9euUWFUtNKu7ZHzrr99AQ5gp4VwLQURepJmmlpPEujFc/Oxtn3zM1NybrOA2DKgI/O6Muc0Qqu0rwDsVUnsIfUENHFwmfmHld/Ses6BPrPHlPYfdvrfr3dU7WfvWKdNuPv3bXB8hkGBiT63RXDFt3WULg4oc+zm578fM1uPX1hbj3v0sAqD/PmmTcU4PQ5ffIRf55JwUqFxB2ZFpji7ipmAlzjRi/6Agd9XMA52reR0AoH9NfXdZFwicRUW8/iPxJgx+CLqHk7HnB/xIJxihq+W/eZVUKVm1ycbNw/c7s3/VNuQktRurPszoZ9+xtriu0Nqh+mY6sZu5Xatzt7W8xJqYI8Yt11jWXBsU666gS6W70Mn/tDqzf4V3mXNm4sVhg7oWOnA8ufN1NdTKahH7K0aKST2TcdOlX5VUHlz02HQCwvbFFy/eL7tsGx+StY4VbnYxjR1NrgeP/81XbsHFns5Zxg5ojpTVEMXEN4q3564XvmzDXiPHzQehoiQk4VFdbMM1Zvb3gJtTlpJ6xYhsA4OS/vo8JN09WPIpgXJ/CbtpMTA6zIb92XDDwyU11KWyniam+2VpN84WHX5VXHXxqX2Nd1kTR+XzicKTqWOHylfVht7ydt/20uz7Ej578FIBOE5NfyKkeDeLtBRvwwaJN2e2LN+zEgnU7TJhr1Fghkf5Zt6ovSHZlksngy431OPXOD/CH1xbkxtXgiOLq8oufrcGrs9cK9/FqDVrSuBI+HtXiMBmPocWOEuOnxFfxrZpCIp0CYu6aHQAs2/X2hhZMX7bV62Na0RWxJtKI3pq/Ifu3Dg2iJpX7fVvTGSzfvKtgHx2Z1Hw8N4xZfh6dmuirc3LP6fF/eQ8n3f4+tje2mGquUVKViGGzR8ipjiYvQL5tc4ttL+arPkBPRzlnVMnkBRuE+6Q0GK2TPmUZODoiinY2t2Jbw27ssk0fG3Y2gzGWPRbVUTZO4frXyYsAWNd37A1vKB0nDEHZ5MUSpA81ePgKSsGZ83Hr6wtx73tL8NZVR+bto75Ynzi5tHF3Ovvb6nQYP/HRioJt9U2tZelJ3WEFRCrhU/mT5ZspVOHskyBqPK9Tg+DfL6IqGdyovZhxvVuO6jG98Ml63A1vZrdd8+xsNLWkMX5IDwDqH2yRcHU6bsuBLgFRlYje4OC8N+99z4qUci6qAA0+CHsRwX1KTS1pVCfjOOwPb2cXdlFHFO1uzWgPPhDRYU1MVYm4Z9kL3Vm36QxzNH3JTZY6bJtO9dvrntahQcR96vZw1Gto4vP43Yvzsp38VJvTUoJJc2dzeQVEsyYnhDuDOwpEYy7fnN+2V0epDcAKHV61tQF7Xfca/vnRiqxwAKLvzdDQki5LFFPH1SB8CrsxTU7qhCNpTRTRxAD1s6aD/zdjVfbvBx1lEkSTXKm0pDN4c956PPD+EsxcsRWvzF6HZbdMAqDv9/U7D94LXKcPolJo0aRBuDO4o0A05l3vLM57rctJDQArbGH0wmerXWMqHTIQE+YaMSlXXaQNO5ow9OqX8c7CDY5WhmpJCspe5IUOakiU8+L3jjIJOgQEV8//9s5ivDJ7HQBg6NUvZzNudSCTQazeB1GeR2jK1ceif1dxd8LenSXa2xbByD6dtHyvHzJRZ7ra9ALIPpDuxWRZJmsT5hodqUQMWxta8OvnZgPIRaA89MFSrd3HAEuDcIdfArxYX/Q3gQ4TEz+tnS57/JKNu7QmygWhOsy1e11S6ffJMqBbDV6/Mt9Ze/3XxuCv543DtZP21jLm/319P0zar7+W7/ZC5mrpMjEBOUe1219ZDgERdV4N0IEFBHe4PT5tBeas3o5vP/wJAOD9RZvw9CcrAehoaGONedbdU3DaXR8CABZtqMcTHy0HwDvKRY8ODaLVFV7KOeNvH2LR+noA6gUwTzbyQ3Wdop51wav1g4Z0Vzomp0t1EkeN7g0A2HdgF3xz4hCcPm4galN6LMd1VQmcddDA7OsJw3rgsUsP1jIWR2YiVt8PIvc8NNj+pJbW/Pu4HGUvdGrfXnRYAeGcFE+984O89/7y5hcA1Ds0vSana/8zB7e/9YWWct8A8O8fHOr7vg4B4VVmAwD+/u6XAHJ5IaqolRAQyqOYPH67OsexHDqip9IxnVTbvSG+e8TwSMxdR4/ugxG96wBYQRVHjOqNv543Ttt4MhpfXONzyrPSneVFgPKYe3aUITrOCAgfVDvl/CbE299ahNY00/KQB61gdYQvBuVAAOpNWzLnoeP3venMffHkdyfmbfvmxCEYN7gbgEIzm0qG9bL8Amu26S+fAlgT48++uieAnHZ4+riBfh8piQnDeuJbE4ega423KU99LabcPXL1s7MjGRMA/vndCXmvrz0l31S402gQ0VGVCJ78VU+cQRpJc2u6PLHmOqKYJLJ5VWsuIgf0JYcOzXutwzRw4YQhOMSlJbRmGC6YsAcAvaaBy48ZgbMOHISzDtQ3Sbvh90sUTajiMcLvz9gXz/7QWwtWXby2U3WwiU6HAnHoiF6Y/LOjsq/dz8eORqNBREadhDlCdQJZUARNc0tGSnAVwy1f369g2/6DugLQY2IqhwAWmSPc56arV4KTMf274LIjh+OU/frjqNG9ceXxo7WN1bk6iT9/Yyz6dBFHNOmAX1tne8ybz9wPPz1+lLYxO1dFF5HfRyIKTJeJaUTvTtlFjdt/Vw4fRIfNg+hcHRx9onoCC3Ki1u9uLeg3rIrzDt4D//l0NT5aahVX++t545DOMFz1r8+0RDH161IdWFVU9bgi85Fba1Pt9xDxxP9MQPe6FADgke/odeKWg5F9OmF030647MgR2W1cW9KF6Hk9aZ9+2LCzCd1rU0rHqpMQRjrrIiUdfUUuPXwYWtMZPDJ1ubYMeT86rAYho0aqFhDdfOyogJVAptPExFfTt587DqePG5i94XRoEL/52hjf95NxUr4K23dgV9Qk47j3Wwdlt339wEF5WkMUGkRtVfQJZVHSr2s13rjyKJw2dkBkY1a7Fk5ViRiunbQ3nv3hYVr8SkE5HzqjmBKOkjzXnToGvz1tH21jBVE2AUFEcSL6lIhesl8PI6KPiGgRET1NRGqXBS72G2iZV3507EjPfVSbe7oECAhAb7YqFz7cFNOsUUAcNrIXPvrVcZ7v62iB2rUmifm/PwlfHdM3u21E705YfPMp2egwnQ/218YOwEFDumszE3ZknPlBy26ZhIU3nozBPWq1jffaT47wfV9nFFOuZluuJlwqHsP3jhqubUwvyqlB/ATAfMfrPwC4jTE2CsBWAJfqHLxHXQrLbpmEMw7wdu6pNvfITE46NYjDR/YCgGwG7leGWgXsTt5XT/JT3y7V+NM5Y7V8tx+iZMMethlCZ37TnecfEBhSbCiesYO6YoBH9rhq3KbIX5xoRW4N7FYDQLOJKdsFMmdS+uKmk3HNyXoSIP0oi4AgokEAJgF4wH5NAI4F8Iy9yyMAzojiWPwKkJUnokjf6vOSw4Zh7u9OxEF2ZdMxA7pg2S2TcJgtOHTQKULnoh8X244/XUlkBv08d/lhmHKNt1aqGl47DACO3asPlt0yCRdOtHwtPer0GTiSiXwNopyU62m5HcAvAXS2X/cEsI0xxuO4VgGIJG7PmVw193cnArDqzj88ZZmUIzssC35/EgBgr+teAwC89KPDMaJ3Jxxyy2Rsa2jBLs2VQGUccCqpFAHxo2NH4rtHDJfKtjZUJuUoQ8PhvqsfHGWFFffVGDXmbAtQbiJfIhPRqQA2MMZmODcLdhWKTyK6jIimE9H0jRs3lnw8Tpt/XVUCdVUJXP+1MfjoV8f5JueUMp57zJpUHP93phWG2q22PLV9dFHnctjOvO4EqYznUnGb84jICAdD0fD7iYi0CgfA2Zq4/AKiHMu7wwCcRkSnAKgG0AWWRtGNiBK2FjEIwBrRhxlj9wG4DwDGjx9fsg4mMiNFcRNw+Mrk5P36462rjsLwXnWRjBsVnV3RYj3qUph6zXGevThUMfPXJ3g2hDIYwhJl7aWhPa05YETv6Kvnuolcg2CMXcMYG8QYGwrgPABvM8YuBPAOgLPt3S4G8HwUx1NOtRXIj4YY2adTWWq86MRp0vrgf48BYEUb9emsVwB3rU1qK3tt6HhEKSCOHN0b//nhobj4kKGRjelFJeVB/C+Aq4hoMSyfxINlPp5IKEcj8ihxCohB3fWFJRoMOuAavuo+IkEcsEf3ilgsltWDyBh7F8C79t9LALS/tFMPUnGrJ3Y5ygZHSZ2JGjK0YVKJGFp3p5XXe2ordNDTLj/D7ZLJjJU/lE0n7V0AGto3PIm0HA2CKgGzvAPwzPcPwdaGaAthPfKdg/HK7LWRFlkrF7edOxZj+nct92EYDKHpVpPEtoaWsnRzqwSoLa9gx48fz6ZPn17uwzAYDO2UFZsb8MJnq3H5MSPLHtCiEiKawRgbH7Sf0SAMBoPBgz161uKKY/WVMa90jA/CYDAYDEKMgDAYDAaDECMgDAaDwSDECAiDwWAwCDECwmAwGAxCjIAwGAwGgxAjIAwGg8EgxAgIg8FgMAhp05nURLQRwPIiP94LwCaFh9MWMOfcMTDn3DEo5ZyHMMZ6B+3UpgVEKRDRdJlU8/aEOeeOgTnnjkEU52xMTAaDwWAQYgSEwWAwGIR0ZAFxX7kPoAyYc+4YmHPuGGg/5w7rgzAYDAaDPx1ZgzAYDAaDDx1SQBDRSUS0kIgWE9HV5T4eVRDRYCJ6h4jmE9FcIvqJvb0HEb1JRIvs/7vb24mI7rB/h8+J6MDynkFxEFGciD4lopfs18OI6CP7fJ8mopS9vcp+vdh+f2g5j7sUiKgbET1DRAvs631Ie77ORHSlfU/PIaIniai6PV5nInqIiDYQ0RzHttDXlYgutvdfREQXF3s8HU5AEFEcwN8AnAxgDIDziWhMeY9KGa0AfsYY2xvARACX2+d2NYDJjLFRACbbrwHrNxhl/7sMwN3RH7ISfgJgvuP1HwDcZp/vVgCX2tsvBbCVMTYSwG32fm2VvwJ4jTG2F4CxsM6/XV5nIhoI4McAxjPG9gUQB3Ae2ud1fhjASa5toa4rEfUAcD2ACQAOBnA9FyqhYYx1qH8ADgHwuuP1NQCuKfdxaTrX5wGcAGAhgP72tv4AFtp/3wvgfMf+2f3ayj8Ag+yH5lgALwEgWMlDCff1BvA6gEPsvxP2flTucyjinLsAWOo+9vZ6nQEMBLASQA/7ur0E4MT2ep0BDAUwp9jrCuB8APc6tuftF+Zfh9MgkLvZOKvsbe0KW60+AMBHAPoyxtYCgP1/H3u39vBb3A7glwAy9uueALYxxlrt185zyp6v/f52e/+2xnAAGwH8wzatPUBEdWin15kxthrAnwCsALAW1nWbgfZ/nTlhr6uy690RBYSo83i7CuUiok4A/g3gp4yxHX67Cra1md+CiE4FsIExNsO5WbArk3ivLZEAcCCAuxljBwDYhZzZQUSbPm/bPHI6gGEABgCog2VecdPernMQXuep7Pw7ooBYBWCw4/UgAGvKdCzKIaIkLOHwBGPsWXvzeiLqb7/fH8AGe3tb/y0OA3AaES0D8BQsM9PtALoRUcLex3lO2fO13+8KYEuUB6yIVQBWMcY+sl8/A0tgtNfrfDyApYyxjYyxFgDPAjgU7f86c8JeV2XXuyMKiE8AjLIjIFKwnF0vlPmYlEBEBOBBAPMZY39xvPUCAB7JcDEs3wTffpEdDTERwHauyrYFGGPXMMYGMcaGwrqObzPGLgTwDoCz7d3c58t/h7Pt/dvcypIxtg7ASiLa0950HIB5aKfXGZZpaSIR1dr3OD/fdn2dHYS9rq8D+CoRdbe1r6/a28JTbodMmZxApwD4AsCXAK4t9/EoPK/DYamSnwOYZf87BZb9dTKARfb/Pez9CVZE15cAZsOKEin7eRR57kcDeMn+eziAjwEsBvD/AFTZ26vt14vt94eX+7hLON9xAKbb1/o5AN3b83UG8DsACwDMAfAYgKr2eJ0BPAnLz9ICSxO4tJjrCuA79vkvBvDtYo/HZFIbDAaDQUhHNDEZDAaDQQIjIAwGg8EgxAgIg8FgMAgxAsJgMBgMQoyAMBgMBoMQIyAM7Ro7RvxtIupivy6olhnw+aOJ6FBFx3ITEa0konrX9kuIaCMRzbL//U8JY5xGRVYoJqK3ii7qZmiXmDBXQ0VDRL+FVZmW19xJAJhm/12wnTH2W9fnJwE4njF2pf36SAD1AB5lVmVQmfHrGWN/KulErO+aCGA5gEWMsU6O7ZfAimG/otQxSsEuCz2IMXZTOY/DUDkYDcLQFjiPMXYqY+xUWBnTQdudXIhc5ikYY+9BUHaBiN4lotuJaIrdc+Bgu+Dh9wFcaa/sjyjlJBhj01iRGcxENJSs3g8P2Mf3BBEdT0Qf2jX/D7b3u4SI7rL/ftjuFzCFiJYQ0dn29v5E9J59TnMc5/UCrEqgBgMAa9VlMLRnDgPwPcl96xhjh9paxkOMsX2J6B54aBBEdAysfgNuGhhjYc1SZ9njfgHgSsbYSsE+IwGcA6v2/ycALoCVPX8agF8BOEPwmf72PnvBEgDP2J97nTF2k90fpRYAGGNbyWq205Mxtjnk8RvaIUZAGNo7PRhjOyX3fRKwtAwi6kJE3fx2Zoy9A6vkRam8COBJxlgzEX0fwCOwCg+6WcoYmw0ARDQXVhMZRkSzYfUQEPEcYywDYB4R9bW3fQLgIbuw43OMsVmO/TfAqphqBITBmJgM7Z5WIpK9z90OOV8HHREd43AsO/9NCXOAjLHNjLFm++X9AA7y2LXZ8XfG8ToD78We8zNkj/cegCMBrAbwGBFd5NinGkCj/NEb2jNGgzC0dxbCKuq2WGLfcwG8Q0SHw6qMuZ2IdsLq4FaAKg2CiPo7fBOnIb99qnKIaAiA1Yyx+8lqNHQggEftSqn9ACzTOb6h7WA0CEN752VYlV4BAET0JICpAPYkolVEdKlj36326v8e5PobvwjgTBVOaiL6IxGtAlBrj/1b+60fE9FcIvoMVu/lS0oZR4KjAcwiok8BnAWrvzVgaS7TWK5Lm6GDY8JcDRWNPYnezhjbZr/uBuCn9tsF2wVhrv1hhbSeEDDOuwB+zhibrvQE2hBE9FcALzDGJpf7WAyVgTExGSqdDbDMH7zndAzAa/bfXtuzMMbWEtH9RNSF+bdfNQBzjHAwODEahMFgMBiEGB+EwWAwGIQYAWEwGAwGIUZAGAwGg0GIERAGg8FgEGIEhMFgMBiEGAFhMBgMBiH/H3Id2xWiMgOlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NUM_ENTRIES = 1000\n",
    "plt.plot(range(NUM_ENTRIES), data[0:NUM_ENTRIES])\n",
    "plt.ylabel(\"전기소비량\")\n",
    "plt.xlabel(\"시간 (1pt = 15 mins)\")\n",
    "plt.show()\n",
    "\n",
    "np.save(os.path.join(DATA_DIR, \"LD_250.npy\"), np.array(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import math\n",
    "import os\n",
    "\n",
    "data = np.load(os.path.join(DATA_DIR, \"LD_250.npy\"))\n",
    "STATELESS = True\n",
    "# NUM_TIMESTEPS는 기간 \n",
    "# 다음값을 예측 20개\n",
    "NUM_TIMESTEPS = 20\n",
    "HIDDEN_SIZE = 10\n",
    "BATCH_SIZE = 96\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "data = data.reshape(-1, 1)\n",
    "scaler = MinMaxScaler(feature_range=(0,1), copy=False)\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "X = np.zeros((data.shape[0], NUM_TIMESTEPS))\n",
    "Y = np.zeros((data.shape[0], 1))\n",
    "for i in range(len(data) - NUM_TIMESTEPS - 1):\n",
    "    X[i] = data[i:i + NUM_TIMESTEPS].T\n",
    "    Y[i] = data[i + NUM_TIMESTEPS + 1]\n",
    "X = np.expand_dims(X, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(98179, 20, 1) (42077, 20, 1) (98179, 1) (42077, 1)\n"
     ]
    }
   ],
   "source": [
    "sp = int(0.7 * len(data))\n",
    "Xtrain, Xtest, Ytrain, Ytest = X[0:sp], X[sp:], Y[0:sp], Y[sp:]\n",
    "print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 98179 samples, validate on 42077 samples\n",
      "Epoch 1/5\n",
      "98179/98179 [==============================] - 9s 96us/step - loss: 0.0075 - mean_squared_error: 0.0075 - val_loss: 0.0038 - val_mean_squared_error: 0.0038\n",
      "Epoch 2/5\n",
      "98179/98179 [==============================] - 8s 82us/step - loss: 0.0045 - mean_squared_error: 0.0045 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 3/5\n",
      "98179/98179 [==============================] - 8s 80us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 4/5\n",
      "98179/98179 [==============================] - 8s 79us/step - loss: 0.0044 - mean_squared_error: 0.0044 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n",
      "Epoch 5/5\n",
      "98179/98179 [==============================] - 8s 80us/step - loss: 0.0043 - mean_squared_error: 0.0043 - val_loss: 0.0036 - val_mean_squared_error: 0.0036\n"
     ]
    }
   ],
   "source": [
    "# 비상태 보존\n",
    "if STATELESS:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(HIDDEN_SIZE, input_shape=(NUM_TIMESTEPS, 1), return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "# 상태 보존 : 마지막 state값을 다음 batch의 초기 state값으로 활용\n",
    "else:\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(HIDDEN_SIZE, stateful=True, batch_input_shape=(BATCH_SIZE, NUM_TIMESTEPS, 1), return_sequences=False))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error'])\n",
    "\n",
    "if STATELESS:\n",
    "    model.fit(Xtrain, Ytrain, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_data=(Xtest, Ytest), shuffle=False)\n",
    "else:\n",
    "    train_size = (Xtrain.shape[0] // BATCH_SIZE) * BATCH_SIZE\n",
    "    test_size = (Xtest.shape[0] // BATCH_SIZE) * BATCH_SIZE\n",
    "    Xtrain, Ytrain = Xtrain[0:train_size], Ytrain[0:train_size]\n",
    "    Xtest, Ytest = Xtest[0:test_size], Ytest[0:test_size]\n",
    "    print(Xtrain.shape, Xtest.shape, Ytrain.shape, Ytest.shape)\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        print(\"EPOCH {:d}/{:d}\".format(i+1, NUM_EPOCHS))\n",
    "        model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE, epochs=1, validation_data=(X_test, Ytest), shuffle=False)\n",
    "        model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42077/42077 [==============================] - 1s 16us/step\n",
      "\n",
      "MSE: 0.004, RMSE: 0.060234\n"
     ]
    }
   ],
   "source": [
    "score, _ = model.evaluate(Xtest, Ytest, batch_size=BATCH_SIZE)\n",
    "rmse = math.sqrt(score)\n",
    "print(\"\\nMSE: {:.3f}, RMSE: {:3f}\".format(score, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "import numpy as np\n",
    "\n",
    "INPUT_FILE = \"./alice_in_wonderland.txt\"\n",
    "\n",
    "fin = open(INPUT_FILE, \"rb\")\n",
    "lines = []\n",
    "for line in fin:\n",
    "    line = line.strip().lower()\n",
    "    line = line.decode(\"ascii\", \"ignore\")\n",
    "    if len(line) == 0:\n",
    "        continue\n",
    "    lines.append(line)\n",
    "fin.close()\n",
    "text = \" \".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set : 키의 집합\n",
    "chars = set([c for c in text])\n",
    "nb_chars = len(chars)\n",
    "char2index = dict((c, i) for i, c in enumerate(chars))\n",
    "index2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10개의 단어를 중심으로 다음에 올 알파벳 예측\n",
    "SEQLEN = 10\n",
    "STEP = 1\n",
    "input_chars = list()\n",
    "label_chars = list()\n",
    "\n",
    "for i in range(0, len(text) - SEQLEN, STEP):\n",
    "    input_chars.append(text[i:i + SEQLEN])\n",
    "    label_chars.append(text[i + SEQLEN])\n",
    "# 위의 데이터를 vectorization\n",
    "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
    "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
    "\n",
    "for i, input_char in enumerate(input_chars):\n",
    "    for j, ch in enumerate(input_char):\n",
    "        X[i, j, char2index[ch]] = 1\n",
    "    y[i, char2index[label_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_ITERATIONS = 25\n",
    "NUM_EPOCHS_PER_ITERATION = 1\n",
    "NUM_PREDS_PER_EPOCH = 100\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, nb_chars), unroll=True))\n",
    "model.add(Dense(nb_chars))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Iteration #: 0\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 43us/step - loss: 1.5435\n",
      "Generating from seed:  mineral, \n",
      " mineral, and the queen the some the courd the same the courd the same the courd the same the courd the same t\n",
      "==================================================\n",
      "Iteration #: 1\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.5266\n",
      "Generating from seed:  like that\n",
      " like that she had not the caterpillar was the gryphon a made the found the courted the door the little beange\n",
      "==================================================\n",
      "Iteration #: 2\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.5111\n",
      "Generating from seed: se! who ev\n",
      "se! who every with the terms of the time of the remain of the loot a mores in a long of the terms of the time \n",
      "==================================================\n",
      "Iteration #: 3\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.4968\n",
      "Generating from seed: ater, and \n",
      "ater, and she was a little the thing the mock turtle the remark, and she was a little the thing the mock turtl\n",
      "==================================================\n",
      "Iteration #: 4\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.4838\n",
      "Generating from seed: you cut yo\n",
      "you cut you as she had not a more what i should the mouse of the gryphon, and the mores as it was so as she ha\n",
      "==================================================\n",
      "Iteration #: 5\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 46us/step - loss: 1.4713\n",
      "Generating from seed: n. we had \n",
      "n. we had the dormouse in a low of onereation of the white rabbit alice had been the some when i she could not\n",
      "==================================================\n",
      "Iteration #: 6\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 46us/step - loss: 1.4606\n",
      "Generating from seed: lice, feel\n",
      "lice, feel alice that in the soon the court in the soon the court in the soon the court in the soon the court \n",
      "==================================================\n",
      "Iteration #: 7\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.4504\n",
      "Generating from seed: eriodic ta\n",
      "eriodic tand her hand alice was no one of the soon and hand alice was no one of the soon and hand alice was no\n",
      "==================================================\n",
      "Iteration #: 8\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 46us/step - loss: 1.4415\n",
      "Generating from seed: er at the \n",
      "er at the caterpillar of the caterpillar of the caterpillar of the caterpillar of the caterpillar of the cater\n",
      "==================================================\n",
      "Iteration #: 9\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 44us/step - loss: 1.4327\n",
      "Generating from seed:  *    *   \n",
      " *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *   \n",
      "==================================================\n",
      "Iteration #: 10\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.4244\n",
      "Generating from seed: ace, and a\n",
      "ace, and a great came and were she was a great came and were she was a great came and were she was a great cam\n",
      "==================================================\n",
      "Iteration #: 11\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.4180\n",
      "Generating from seed: urled roun\n",
      "urled round at the dormouse into the dormouse into the dormouse into the dormouse into the dormouse into the d\n",
      "==================================================\n",
      "Iteration #: 12\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.4109\n",
      "Generating from seed: d round, e\n",
      "d round, end of the party of the project gutenberg literary archild into the door the pigeon in an any project\n",
      "==================================================\n",
      "Iteration #: 13\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.4035\n",
      "Generating from seed: and they r\n",
      "and they rad to the gryphon its a stations and the terms of the time the mory that it was a lotice the mock tu\n",
      "==================================================\n",
      "Iteration #: 14\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.3976\n",
      "Generating from seed: and this t\n",
      "and this time the works in the one of the same to the door of the same to the door of the same to the door of \n",
      "==================================================\n",
      "Iteration #: 15\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.3906\n",
      "Generating from seed: d vanished\n",
      "d vanished a pitted the queen, and which which the more sor began to the queen, and which which the more sor b\n",
      "==================================================\n",
      "Iteration #: 16\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 44us/step - loss: 1.3855\n",
      "Generating from seed: tting on a\n",
      "tting on a little thing a little thing a little thing a little thing a little thing a little thing a little th\n",
      "==================================================\n",
      "Iteration #: 17\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 44us/step - loss: 1.3808\n",
      "Generating from seed:  difficult\n",
      " difficult know the mock turtle some to say the queen she went on a dormouse in a low little and go on the roo\n",
      "==================================================\n",
      "Iteration #: 18\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.3762\n",
      "Generating from seed: ecutions i\n",
      "ecutions in the project gutenberg-tm electronic works and she was any looked at the project gutenberg-tm elect\n",
      "==================================================\n",
      "Iteration #: 19\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.3708\n",
      "Generating from seed:  said the \n",
      " said the door the sent of the sed her head a rouse a little alice was a little alice was a little alice was a\n",
      "==================================================\n",
      "Iteration #: 20\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.3661\n",
      "Generating from seed: nding that\n",
      "nding that was no one of the hatter was the gryphon alice was not any reash the nors the dormouse in the mouse\n",
      "==================================================\n",
      "Iteration #: 21\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 44us/step - loss: 1.3617\n",
      "Generating from seed:  said to t\n",
      " said to the door as it into a minute or two the dong the hatter was a long to the door as it into a minute or\n",
      "==================================================\n",
      "Iteration #: 22\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 45us/step - loss: 1.3571\n",
      "Generating from seed: of course-\n",
      "of course--and a little be the court in a find beast be a little be the court in a find beast be a little be t\n",
      "==================================================\n",
      "Iteration #: 23\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 44us/step - loss: 1.3535\n",
      "Generating from seed: ur watch t\n",
      "ur watch turn the pirton the parmouth was a little be the parmouth was a little be the parmouth was a little b\n",
      "==================================================\n",
      "Iteration #: 24\n",
      "Epoch 1/1\n",
      "158773/158773 [==============================] - 7s 44us/step - loss: 1.3505\n",
      "Generating from seed: t a very s\n",
      "t a very such a very such a very such a very such a very such a very such a very such a very such a very such \n"
     ]
    }
   ],
   "source": [
    "for iteration in range(NUM_ITERATIONS):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Iteration #: %d\" % iteration)\n",
    "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
    "    test_idx = np.random.randint(len(input_chars))\n",
    "    test_chars = input_chars[test_idx]\n",
    "    print(\"Generating from seed: %s\" % test_chars)\n",
    "    print(test_chars, end=\"\")\n",
    "    for i in range(NUM_PREDS_PER_EPOCH):\n",
    "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
    "        for i, ch in enumerate(test_chars):\n",
    "            Xtest[0, i, char2index[ch]] = 1\n",
    "        pred = model.predict(Xtest, verbose=0)[0]\n",
    "        ypred = index2char[np.argmax(pred)]\n",
    "        print(ypred, end=\"\")\n",
    "        test_chars = test_chars[1:] + ypred\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
